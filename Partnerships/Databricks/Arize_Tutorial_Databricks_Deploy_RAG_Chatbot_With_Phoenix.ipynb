{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131c1180-55ea-43d7-a4f9-7cfaa7c39fc8",
     "showTitle": false,
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-llm-rag-chatbot-jason_bricks_std` from the dropdown menu ([open cluster configuration](https://dbc-458303b2-a0c9.cloud.databricks.com/#setting/clusters/0112-071913-b6023fae/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('llm-rag-chatbot')` or re-install the demo: `dbdemos.install('llm-rag-chatbot')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e916460-10ee-459f-bdac-6a6ec90f4212",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Creating the chatbot with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-flow-2.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Our Vector Search Index is now ready!\n",
    "\n",
    "Let's now create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1126084104782633&notebook=%2F01-quickstart%2F02-Deploy-RAG-Chatbot-Model&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F01-quickstart%2F02-Deploy-RAG-Chatbot-Model&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe40fd4-6308-467f-978e-90232652e68b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note: RAG performs document searches using Databricks Vector Search. In this notebook, we assume that the search index is ready for use. Make sure you run the previous [01-Data-Preparation-and-Index]($./01-Data-Preparation-and-Index [DO NOT EDIT]) notebook.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae89bbd6-3872-4b2a-830a-9e1e8c6fafef",
     "showTitle": true,
     "title": "Install the required libraries"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nCollecting mlflow==2.9.0\n  Downloading mlflow-2.9.0-py3-none-any.whl (19.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 41.6 MB/s eta 0:00:00\nCollecting langchain==0.0.344\n  Downloading langchain-0.0.344-py3-none-any.whl (1.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 63.6 MB/s eta 0:00:00\nCollecting databricks-vectorsearch==0.22\n  Downloading databricks_vectorsearch-0.22-py3-none-any.whl (8.5 kB)\nCollecting databricks-sdk==0.12.0\n  Downloading databricks_sdk-0.12.0-py3-none-any.whl (301 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.7/301.7 kB 27.3 MB/s eta 0:00:00\nCollecting mlflow[databricks]\n  Downloading mlflow-2.9.2-py3-none-any.whl (19.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 54.2 MB/s eta 0:00:00\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (8.0.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (4.11.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.4.39)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (2022.7)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (3.1.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (0.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (2.28.1)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (22.0)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (20.1.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (2.0.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (0.4.2)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (3.7.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (6.0)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 17.1 MB/s eta 0:00:00\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.1.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.10.0)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.5.3)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (3.4.1)\nCollecting alembic!=1.10.0,<2\n  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 25.2 MB/s eta 0:00:00\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (4.24.0)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.23.5)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (2.2.5)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (0.18.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (3.1.27)\nRequirement already satisfied: pyarrow<15,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (8.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (0.6.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.33)\nCollecting langchain-core<0.1,>=0.0.8\n  Downloading langchain_core-0.0.13-py3-none-any.whl (188 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.2/188.2 kB 20.3 MB/s eta 0:00:00\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (0.0.64)\nRequirement already satisfied: anyio<4.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (3.5.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (8.1.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (4.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (3.8.6)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.10.6)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (2.8.1)\nCollecting mlflow[databricks]\n  Downloading mlflow-2.9.1-py3-none-any.whl (19.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 36.4 MB/s eta 0:00:00\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (1.24.28)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (12.14.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.9.0) (2.11.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (22.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (2.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.4.0)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.9.0) (4.4.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.9.0) (1.2.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.344) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.344) (1.2.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.9.0) (12.19.0)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.9.0) (1.29.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.9.0) (0.6.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.9.0) (0.10.0)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.9.0) (0.6.2)\nRequirement already satisfied: botocore<1.28.0,>=1.27.28 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.9.0) (1.27.96)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.9.0) (1.26.14)\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.9.0) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.9.0) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.9.0) (2.3.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.9.0) (0.8.10)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (0.9.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (3.20.1)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow==2.9.0) (0.58.0)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.9.0) (2.2.2)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.9.0) (2.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow==2.9.0) (4.0.11)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.9.0) (2.21.0)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.9.0) (2.6.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.9.0) (2.14.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.9.0) (2.3.3)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow==2.9.0) (65.6.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.9.0) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.9.0) (2.1.1)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.344) (2.4)\nCollecting packaging<24\n  Using cached packaging-23.2-py3-none-any.whl (53 kB)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (1.4.4)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (4.25.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (0.11.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (1.0.5)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.9.0) (9.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.9.0) (2022.12.7)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.9.0) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.9.0) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.9.0) (2.0.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.9.0) (39.0.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.9.0) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow==2.9.0) (1.61.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.9.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.9.0) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.9.0) (5.3.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage>=1.30.0->mlflow==2.9.0) (1.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (0.4.3)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.9.0) (1.15.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.9.0) (0.4.8)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.9.0) (2.21)\nInstalling collected packages: querystring-parser, packaging, docker, databricks-sdk, alembic, mlflow, langchain-core, langchain, databricks-vectorsearch\n  Attempting uninstall: packaging\n    Found existing installation: packaging 22.0\n    Not uninstalling packaging at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'packaging'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.1.6\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.0.314\n    Not uninstalling langchain at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'langchain'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.16.3 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed alembic-1.13.1 databricks-sdk-0.12.0 databricks-vectorsearch-0.22 docker-6.1.3 langchain-0.0.344 langchain-core-0.0.13 mlflow-2.9.0 packaging-23.2 querystring-parser-1.2.4\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow==2.9.0 langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 mlflow[databricks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb63b50-d417-4b0d-a263-79063cdb6308",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Install Arize Phoenix AI Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb36baec-8595-4fb0-a853-98895d3fe03c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is a LLM-powered application utilizes LangChain. Arize Phoenix captures LangChain span and trace information helping with debugging of LangChain LLM calls. \n",
    "\n",
    "In addition to LangChain, Phoenix supports instrumentation using OTEL LLM tracing, LlamaIndex, LangChain and general dataframe analysis\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "LLM Traces are a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context (such as retrieval from vector stores, usage of external tools, etc).\n",
    "\n",
    "Traces are made up of a sequence of spans. A span represents a unit of work or operation (think a span of time).\n",
    "\n",
    "LLM Evaluations help get visbility into the performance of the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3cec8e-ea51-4a2c-bec7-488e2be8df05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nCollecting typing-extensions==4.7.1\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nInstalling collected packages: typing-extensions\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed typing-extensions-4.7.1\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nCollecting arize-phoenix\n  Downloading arize_phoenix-2.5.0-py3-none-any.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 9.6 MB/s eta 0:00:00\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (1.10.0)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (3.1.2)\nCollecting starlette\n  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.1/71.1 kB 11.3 MB/s eta 0:00:00\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (4.64.1)\nRequirement already satisfied: protobuf<5.0,>=3.20 in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (4.24.0)\nRequirement already satisfied: wrapt in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (1.14.1)\nCollecting opentelemetry-sdk\n  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.6/105.6 kB 17.1 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (5.9.0)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (1.23.5)\nCollecting opentelemetry-proto\n  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.8/50.8 kB 8.9 MB/s eta 0:00:00\nCollecting strawberry-graphql==0.208.2\n  Downloading strawberry_graphql-0.208.2-py3-none-any.whl (269 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.0/269.0 kB 14.7 MB/s eta 0:00:00\nCollecting ddsketch\n  Downloading ddsketch-2.0.4-py3-none-any.whl (18 kB)\nCollecting sortedcontainers\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nCollecting umap-learn\n  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.9/90.9 kB 14.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (1.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60/lib/python3.10/site-packages (from arize-phoenix) (4.7.1)\nRequirement already satisfied: scikit-learn<1.3.0 in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (1.1.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (2.28.1)\nCollecting hdbscan<1.0.0,>=0.8.33\n  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 28.7 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting uvicorn\n  Downloading uvicorn-0.26.0-py3-none-any.whl (60 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.5/60.5 kB 10.7 MB/s eta 0:00:00\nRequirement already satisfied: pyarrow in /databricks/python3/lib/python3.10/site-packages (from arize-phoenix) (8.0.0)\nCollecting graphql-core<3.3.0,>=3.2.0\n  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202.9/202.9 kB 30.6 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /databricks/python3/lib/python3.10/site-packages (from strawberry-graphql==0.208.2->arize-phoenix) (2.8.2)\nRequirement already satisfied: cython<3,>=0.27 in /databricks/python3/lib/python3.10/site-packages (from hdbscan<1.0.0,>=0.8.33->arize-phoenix) (0.29.32)\nRequirement already satisfied: joblib>=1.0 in /databricks/python3/lib/python3.10/site-packages (from hdbscan<1.0.0,>=0.8.33->arize-phoenix) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<1.3.0->arize-phoenix) (2.2.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from ddsketch->arize-phoenix) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->arize-phoenix) (2.1.1)\nCollecting opentelemetry-api==1.22.0\n  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.9/57.9 kB 9.6 MB/s eta 0:00:00\nCollecting opentelemetry-semantic-conventions==0.43b0\n  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\nCollecting deprecated>=1.2.6\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\nCollecting importlib-metadata<7.0,>=6.0\n  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->arize-phoenix) (2022.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->arize-phoenix) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->arize-phoenix) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->arize-phoenix) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->arize-phoenix) (2.0.4)\nRequirement already satisfied: anyio<5,>=3.4.0 in /databricks/python3/lib/python3.10/site-packages (from starlette->arize-phoenix) (3.5.0)\nRequirement already satisfied: numba>=0.51.2 in /databricks/python3/lib/python3.10/site-packages (from umap-learn->arize-phoenix) (0.56.4)\nCollecting pynndescent>=0.5\n  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 kB 8.8 MB/s eta 0:00:00\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 10.5 MB/s eta 0:00:00\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from uvicorn->arize-phoenix) (8.0.4)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.2.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /databricks/python3/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn->arize-phoenix) (0.39.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn->arize-phoenix) (65.6.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api==1.22.0->opentelemetry-sdk->arize-phoenix) (3.11.0)\nBuilding wheels for collected packages: hdbscan, umap-learn\n  Building wheel for hdbscan (pyproject.toml): started\n  Building wheel for hdbscan (pyproject.toml): finished with status 'done'\n  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3031935 sha256=07fe4af373fb2561f238ccd8874e648ec0b9a053673e49ea2c6bcfe5d57f4bef\n  Stored in directory: /root/.cache/pip/wheels/e3/c7/93/1036606950c902cb2f095fc78d4614c531c52e0dc55459e39f\n  Building wheel for umap-learn (setup.py): started\n  Building wheel for umap-learn (setup.py): finished with status 'done'\n  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86837 sha256=92f68971e882f5434c743a8adff3d814a83aa9ed35d36bcb19e4d538a5bb8444\n  Stored in directory: /root/.cache/pip/wheels/e5/c1/ee/d66f7fa7b04af4405e561e6836c203b5eea7e85c4a6fa29946\nSuccessfully built hdbscan umap-learn\nInstalling collected packages: sortedcontainers, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, h11, graphql-core, deprecated, ddsketch, uvicorn, strawberry-graphql, starlette, opentelemetry-api, pynndescent, opentelemetry-sdk, hdbscan, umap-learn, arize-phoenix\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 4.11.3\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.16.3 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed arize-phoenix-2.5.0 ddsketch-2.0.4 deprecated-1.2.14 graphql-core-3.2.3 h11-0.14.0 hdbscan-0.8.33 importlib-metadata-6.11.0 opentelemetry-api-1.22.0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 pynndescent-0.5.11 sortedcontainers-2.4.0 starlette-0.35.1 strawberry-graphql-0.208.2 umap-learn-0.5.5 uvicorn-0.26.0\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nRequirement already satisfied: openai in /databricks/python3/lib/python3.10/site-packages (0.28.1)\nCollecting openai\n  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.4/223.4 kB 4.1 MB/s eta 0:00:00\nRequirement already satisfied: tqdm>4 in /databricks/python3/lib/python3.10/site-packages (from openai) (4.64.1)\nCollecting httpx<1,>=0.23.0\n  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 7.0 MB/s eta 0:00:00\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai) (1.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60/lib/python3.10/site-packages (from openai) (4.7.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.10/site-packages (from openai) (3.5.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.10/site-packages (from openai) (1.10.6)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\nCollecting httpcore==1.*\n  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 7.4 MB/s eta 0:00:00\nRequirement already satisfied: h11<0.15,>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nInstalling collected packages: httpcore, httpx, openai\n  Attempting uninstall: openai\n    Found existing installation: openai 0.28.1\n    Not uninstalling openai at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'openai'. No files were found to uninstall.\nSuccessfully installed httpcore-1.0.2 httpx-0.26.0 openai-1.9.0\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nRequirement already satisfied: nest_asyncio in /databricks/python3/lib/python3.10/site-packages (1.5.6)\nCollecting nest_asyncio\n  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\nInstalling collected packages: nest_asyncio\n  Attempting uninstall: nest_asyncio\n    Found existing installation: nest-asyncio 1.5.6\n    Not uninstalling nest-asyncio at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-43c61980-4d46-44f8-a334-26be5a687d60\n    Can't uninstall 'nest-asyncio'. No files were found to uninstall.\nSuccessfully installed nest_asyncio-1.6.0\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nRequirement already satisfied: tiktoken in /databricks/python3/lib/python3.10/site-packages (0.5.1)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken) (2022.7.9)\nRequirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install typing-extensions==4.7.1\n",
    "!pip install arize-phoenix\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade nest_asyncio\n",
    "!pip install tiktoken\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03dadaf-a241-4b37-a776-d0254b5bc62e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CATALOG `main`\nusing catalog.database `main`.`rag_chatbot`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baec18c8-46c4-4163-a731-b1aa63333420",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "  \n",
    "###  This demo requires a secret to work:\n",
    "Your Model Serving Endpoint needs a secret to authenticate against your Vector Search Index (see [Documentation](https://docs.databricks.com/en/security/secrets/secrets.html)).  <br/>\n",
    "**Note: if you are using a shared demo workspace and you see that the secret is setup, please don't run these steps and do not override its value**<br/>\n",
    "\n",
    "- You'll need to [setup the Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/install.html) on your laptop or using this cluster terminal: <br/>\n",
    "`pip install databricks-cli` <br/>\n",
    "- Configure the CLI. You'll need your workspace URL and a PAT token from your profile page<br>\n",
    "`databricks configure`\n",
    "- Create the dbdemos scope:<br/>\n",
    "`databricks secrets create-scope dbdemos`\n",
    "- Save your service principal secret. It will be used by the Model Endpoint to autenticate. If this is a demo/test, you can use one of your [PAT token](https://docs.databricks.com/en/dev-tools/auth/pat.html).<br>\n",
    "`databricks secrets put-secret dbdemos rag_sp_token`\n",
    "\n",
    "*Note: Make sure your service principal has access to the Vector Search index:*\n",
    "\n",
    "```\n",
    "spark.sql('GRANT USAGE ON CATALOG <catalog> TO `<YOUR_SP>`');\n",
    "spark.sql('GRANT USAGE ON DATABASE <catalog>.<db> TO `<YOUR_SP>`');\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, <index_name>, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"<YOUR_SP>\")])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b852f75f-eefd-4805-91ff-6e44aacafb43",
     "showTitle": true,
     "title": "Make sure your SP has read access to your Vector Search Index"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret and permissions seems to be properly setup, you can continue the demo!\n"
     ]
    }
   ],
   "source": [
    "index_name = f\"{catalog}.{db}.databricks_documentation_vs_index\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "test_demo_permissions(\n",
    "    host,\n",
    "    secret_scope=\"dbdemos\",\n",
    "    secret_key=\"rag_sp_token\",\n",
    "    vs_endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=index_name,\n",
    "    embedding_endpoint_name=\"databricks-bge-large-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a16913a-bffa-4ba6-b58c-5622652a6d85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Langchain retriever\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Let's start by building our Langchain retriever. \n",
    "\n",
    "It will be in charge of:\n",
    "\n",
    "* Creating the input question embeddings (with Databricks `bge-large-en`)\n",
    "* Calling the vector search index to find similar documents to augment the prompt with\n",
    "\n",
    "Databricks Langchain wrapper makes it easy to do in one step, handling all the underlying logic and API call for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86cde2d-32c2-48f6-9450-eb28e8a28605",
     "showTitle": true,
     "title": "Setup authentication for our model"
    }
   },
   "outputs": [],
   "source": [
    "# url used to send the request to your model from the serverless endpoint\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = dbutils.secrets.get(\"dbdemos\", \"rag_sp_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1613b064-f253-4f11-8856-5d1eec57815b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings: [0.0186004638671875, -0.0141448974609375, -0.0574951171875, 0.0034027099609375, 0.008453369140625, -0.0216064453125, -0.02471923828125, -0.004688262939453125, 0.0136566162109375, 0.050384521484375, -0.0272064208984375, -0.01470184326171875, 0.054718017578125, -0.0538330078125, -0.01035308837890625, -0.0162200927734375, -0.0188140869140625, -0.017242431640625, -0.051300048828125, 0.0177764892578125]...\n[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\nRelevant documents: page_content='View billable usage using the account console  \\nThis article describes how to use the Usage page in the account console to view usage data across workspaces in your account. You can also download billable usage logs using the Account API.  \\nYou can also access and query billable usage data using Databricks system tables (Public Preview).  \\nYou can view usage in the account console by either of these units:  \\nDatabricks Unit (DBU): A unit of processing capability per hour, billed on per-second usage.  \\nEstimated cost (in $USD currency): Cost in dollars is estimated on a linear cost estimate per DBU based on standard cost-per-DBU pricing for your tier for each type of usage. Actual cost varies depending on your exact contract. Adjust cost-per-DBU for each SKU to change the cost estimate using the settings panel.  \\nAccess the Usage page\\nAccess the Usage page\\nGo to the account console and click the Usage icon.\\n\\nUsage graph' metadata={'id': 4588.0}\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# Test embedding Langchain model\n",
    "# NOTE: your question embedding model must match the one used in the chunk in the previous model\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "print(\n",
    "    f\"Test embeddings: {embedding_model.embed_query('What is Apache Spark?')[:20]}...\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    # Get the vector search index\n",
    "    vsc = VectorSearchClient(\n",
    "        workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"]\n",
    "    )\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"content\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# test our retriever\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.get_relevant_documents(\n",
    "    \"How do I track my Databricks Billing?\"\n",
    ")\n",
    "print(f\"Relevant documents: {similar_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f6683e-73af-41ad-bf6c-e13b7010d8b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Databricks Chat Model to query llama-2-70b-chat foundation model\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-3.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Our chatbot will be using llama2 foundation model to provide answer. \n",
    "\n",
    "While the model is available using the built-in [Foundation endpoint](/ml/endpoints) (using the `/serving-endpoints/databricks-llama-2-70b-chat/invocations` API), we can use Databricks Langchain Chat Model wrapper to easily build our chain.  \n",
    "\n",
    "Note: multipe type of endpoint or langchain models can be used:\n",
    "\n",
    "- Databricks Foundation models (what we'll use)\n",
    "- Your fined-tune model\n",
    "- An external model provider (such as Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d7d50a-8a17-4c28-8024-2343f95c74b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chat model: \nApache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Python, Scala, and R, and an optimized engine that supports general execution graphs. It also provides high-level tools and libraries for data loading, transformation, and machine learning.\n\nSpark is designed to handle large-scale data processing tasks and can process data in real-time or batch mode. It is highly scalable and can handle data processing tasks that are too large for a single machine to handle. It is also highly fault-tolerant, meaning that it can continue to process data even if one or more machines fail.\n\nSpark is widely used in a variety of industries, including finance, healthcare, retail, and telecommunications. It is often used for data warehousing, machine learning, and stream processing.\n\nSome of the key features of Apache Spark include:\n\n1\n"
     ]
    }
   ],
   "source": [
    "# Test Databricks Foundation LLM model\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-llama-2-70b-chat\", max_tokens=200\n",
    ")\n",
    "print(f\"Test chat model: {chat_model.predict('What is Apache Spark')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b847c488-1d80-42fa-8d9c-3db9f9e166c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Arize Phoenix AI Observability to Visualize and Troubleshoot RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e73219-0dae-4e7a-ad71-6a1ea9978923",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 15:22:57.624439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:phoenix.trace.exporter:Arize Phoenix is not running on http://0.0.0.0:6006. Launch Phoenix with `import phoenix as px; px.launch_app()`\n"
     ]
    }
   ],
   "source": [
    "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
    "\n",
    "# If no exporter is specified, the tracer will export to the locally running Phoenix server\n",
    "tracer = OpenInferenceTracer()\n",
    "# If no tracer is specified, a tracer is constructed for you\n",
    "LangChainInstrumentor(tracer).instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869d5561-8ef0-4013-a44b-886ada338af1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit https://dbc-458303b2-a0c9.cloud.databricks.com/driver-proxy/o/1126084104782633/0112-070001-9dtid9cg/6006/\n📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e890b2-59cd-4db2-9948-ffd169ac4845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📺 Opening a view to the Phoenix app. The app is running at https://dbc-458303b2-a0c9.cloud.databricks.com/driver-proxy/o/1126084104782633/0112-070001-9dtid9cg/6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"https://dbc-458303b2-a0c9.cloud.databricks.com/driver-proxy/o/1126084104782633/0112-070001-9dtid9cg/6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7faaef805f90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3fb919-0f5f-4951-8396-e1399e2dfe30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Assembling the complete RAG Chain\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-2.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "\n",
    "Let's now merge the retriever and the model in a single Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4d4583-6321-4ff4-b43a-94a4533c7a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant for Databricks users. You are answering python, coding, SQL, data engineering, spark, data science, DW and platform, API or infrastructure administration question related to Databricks. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94858e0-68a3-4c03-8c81-45846a01c8be",
     "showTitle": true,
     "title": "Let's try our chatbot in the notebook directly:"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can track billing usage on your workspaces by using the Usage page in the Databricks account console. The Usage page allows you to view detailed usage data for your workspaces, including estimated costs in $USD or DBUs. You can use the graph on the page to view usage data by workspace, SKU, or tag, and you can filter the data by date range, workspace, tag, or SKU. Additionally, you can download aggregated or unaggregated usage data by date range. To access the Usage page, go to the account console and click the Usage icon.\n"
     ]
    }
   ],
   "source": [
    "# langchain.debug = True #uncomment to see the chain details and the full prompt being sent\n",
    "question = {\"query\": \"How can I track billing usage on my workspaces?\"}\n",
    "answer = chain.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee71212-b00c-4006-823a-3a8a0a805726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL path for a workspace is located in the Workspace browser. To access the Workspace browser, click Workspace in the sidebar. The URL path is displayed in the address bar of your web browser. It starts with https://<workspace-name>.databricks.com, where <workspace-name> is the name of your Databricks workspace.\n"
     ]
    }
   ],
   "source": [
    "# langchain.debug = True #uncomment to see the chain details and the full prompt being sent\n",
    "question = {\"query\": \"Where is the URL path for a workspace located?\"}\n",
    "answer = chain.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab56f5c-203f-4fb8-b0ff-0c4d65a036d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Arize Phoenix - Run Optional RAG Evaluations \n",
    "Arize supports a suite of LLM Evaluations for retrieval\n",
    "Phoenix has support for a large suite of models for Evals, this example only uses OpenAI\n",
    "\n",
    "This example runs through:\n",
    "\n",
    "Hallucination Eval - Is the answer made up?\n",
    "\n",
    "Retrieval Eval - Is the chunk relevant?\n",
    "\n",
    "Q&A - Is the overall answer correct based on reference text?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c6dc12-2444-4a81-b9a1-7be9d6a3ab2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get traces from Phoenix into dataframe\n",
    "\n",
    "spans_df = px.active_session().get_spans_dataframe()\n",
    "spans_df[\n",
    "    [\n",
    "        \"name\",\n",
    "        \"span_kind\",\n",
    "        \"attributes.input.value\",\n",
    "        \"attributes.retrieval.documents\",\n",
    "    ]\n",
    "].head()\n",
    "\n",
    "from phoenix.session.evaluation import (\n",
    "    get_qa_with_reference,\n",
    "    get_retrieved_documents,\n",
    ")\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.active_session())\n",
    "queries_df = get_qa_with_reference(px.active_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2bc456-dc31-439b-9598-cfbb8e073e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you want to use a different model please see the Phoenix documentation\n",
    "# https://docs.arize.com/phoenix/api/evaluation-models\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Uses your OpenAI API Key\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543fe2bb-db0f-43d2-8238-48f737419e63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For fast Eval runs enable concurrency\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69d1872-6fab-4fda-8efa-d6adba5e8617",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations, DocumentEvaluations\n",
    "from phoenix.experimental.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6f1e14-bf85-4cb8-ac83-9c8b18f4545f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model is working - call GPT-4 turbo\n",
    "model = OpenAIModel(\"gpt-4-1106-preview\", temperature=0.0)\n",
    "model(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c599828c-c41e-439f-b590-8b8b834146bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, let's take a look at how to use LLM evals to evaluate our application.\n",
    "\n",
    "We will be going through a few common evaluation metrics\n",
    "\n",
    "\n",
    "####Ealuation Example Results\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/databricks_notebook_eval2.png\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e757b315-e248-4d02-8de0-f6c8497aaacb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fcb945-d1dd-4400-b392-a70631630521",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.experimental.evals.functions.executor:🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7649503c33eb4f77a9d66c91b1b58442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.experimental.evals.functions.executor:🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb522dab1724bfebde0136604ebb0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\n\rSending Evaluations:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n\rSending Evaluations: 100%|██████████| 4/4 [00:00<00:00, 39.94it/s]\u001b[A\u001b[A\rSending Evaluations: 100%|██████████| 4/4 [00:00<00:00, 38.87it/s]\n"
     ]
    }
   ],
   "source": [
    "if not openai_api_key:\n",
    "    print(\"Skipping Evals as no OpenAI key is configured\")\n",
    "else:\n",
    "    # Creating Hallucination Eval which checks if the application hallucinated\n",
    "    hallucination_eval = llm_classify(\n",
    "        dataframe=queries_df,\n",
    "        model=OpenAIModel(\"gpt-4-1106-preview\", temperature=0.0),\n",
    "        template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "        rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "        provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "        concurrency=4,\n",
    "    )\n",
    "    hallucination_eval[\"score\"] = (\n",
    "        hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n",
    "    ).astype(int)\n",
    "\n",
    "    # Creating Q&A Eval which checks if the application answered the question correctly\n",
    "    qa_correctness_eval = llm_classify(\n",
    "        dataframe=queries_df,\n",
    "        model=OpenAIModel(\"gpt-4-1106-preview\", temperature=0.0),\n",
    "        template=QA_PROMPT_TEMPLATE,\n",
    "        rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "        provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "        concurrency=4,\n",
    "    )\n",
    "\n",
    "    qa_correctness_eval[\"score\"] = (\n",
    "        hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n",
    "    ).astype(int)\n",
    "\n",
    "    # Logs the Evaluations to Phoenix\n",
    "    px.log_evaluations(\n",
    "        SpanEvaluations(\n",
    "            eval_name=\"Hallucination\", dataframe=hallucination_eval\n",
    "        ),\n",
    "        SpanEvaluations(\n",
    "            eval_name=\"QA Correctness\", dataframe=qa_correctness_eval\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79b7fb4-000f-47e7-83bb-940ba65bea4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can use Retrieval Relevance Evals to identify if issues are caused by the retrieval process for RAG. We are going to use an LLM to grade whether or not the chunks retrieved are relevant to the query.\n",
    "\n",
    "####Ealuation Retreival Chunks - Example Results\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/databricks_notebook_retriever_eval.png\"  >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d2b354-c436-47f0-9922-448b4a4a6801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.experimental.evals.functions.executor:🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7c84afb8d04e249e2a39fa602b25ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/8 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\n\n\rSending Evaluations:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n\rSending Evaluations: 100%|██████████| 8/8 [00:00<00:00, 79.87it/s]\u001b[A\u001b[A\u001b[A\rSending Evaluations: 100%|██████████| 8/8 [00:00<00:00, 77.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating Retrieval Relevance Eval\n",
    "\n",
    "from phoenix.experimental.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval = llm_classify(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    model=OpenAIModel(\"gpt-4-1106-preview\", temperature=0.0),\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval[\"score\"] = (\n",
    "    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()]\n",
    "    == \"relevant\"\n",
    ").astype(int)\n",
    "\n",
    "px.log_evaluations(\n",
    "    DocumentEvaluations(\n",
    "        eval_name=\"Relevance\", dataframe=retrieved_documents_eval\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f477f6e2-e2ca-4de3-9827-529928ad11fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Saving our model to Unity Catalog registry\n",
    "\n",
    "Now that our model is ready, we can register it within our Unity Catalog schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87da31d2-9a85-4c62-bab0-76ff71a1a124",
     "showTitle": true,
     "title": "Register our chain to MLFlow"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d624508e1660440a812b7d409bb2710a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'main.rag_chatbot.dbdemos_chatbot_model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5cfa935b5422f9fd0a1fb1b42bfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'main.rag_chatbot.dbdemos_chatbot_model'.\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog}.{db}.dbdemos_chatbot_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"dbdemos_chatbot_rag\") as run:\n",
    "    signature = infer_signature(question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,  # Load the retriever with DATABRICKS_TOKEN env as secret (for authentication).\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808345a9-018c-4436-8233-a06658fe5072",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploying our Chat Model as a Serverless Model Endpoint \n",
    "\n",
    "Our model is saved in Unity Catalog. The last step is to deploy it as a Model Serving.\n",
    "\n",
    "We'll then be able to sending requests from our assistant frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1689d704-8b85-4224-95ff-535553b24a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the endpoint https://dbc-458303b2-a0c9.cloud.databricks.com/ml/endpoints/dbdemos_endpoint_main_rag_chatbot to version 3, this will take a few minutes to package and deploy the endpoint...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/dbdemos_endpoint_main_rag_chatbot\">Model Serving Endpoint page</a> for more details."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedModelInput,\n",
    ")\n",
    "\n",
    "serving_endpoint_name = f\"dbdemos_endpoint_{catalog}_{db}\"[:63]\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=\"Small\",\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",  # <scope>/<secret> that contains an access token\n",
    "                \"PHOENIX_COLLECTOR_ENDPOINT\": \"URL of Phoenix Server\",  # Where to send trace data for Phoenix\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name),\n",
    "    None,\n",
    ")\n",
    "serving_endpoint_url = f\"{host}/ml/endpoints/{serving_endpoint_name}\"\n",
    "if existing_endpoint == None:\n",
    "    print(\n",
    "        f\"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...\"\n",
    "    )\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=serving_endpoint_name, config=endpoint_config\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\"\n",
    "    )\n",
    "    w.serving_endpoints.update_config_and_wait(\n",
    "        served_models=endpoint_config.served_models, name=serving_endpoint_name\n",
    "    )\n",
    "\n",
    "displayHTML(\n",
    "    f'Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{serving_endpoint_name}\">Model Serving Endpoint page</a> for more details.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0128a9a-e538-4308-8b6e-cbab334eaa54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our endpoint is now deployed! You can search endpoint name on the [Serving Endpoint UI](#/mlflow/endpoints) and visualize its performance!\n",
    "\n",
    "Let's run a REST query to try it in Python. As you can see, we send the `test sentence` doc and it returns an embedding representing our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d4f954-c5e7-473d-a97a-e3193c5d9691",
     "showTitle": true,
     "title": "Let's try to send a query to our chatbot"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "answer = w.serving_endpoints.query(\n",
    "    serving_endpoint_name, inputs=[{\"query\": question}]\n",
    ")\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2904b514-4238-4985-8a34-59b7598828f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Let's give it a try, using Gradio as UI!\n",
    "\n",
    "All you now have to do is deploy your chatbot UI. Here is a simple example using Gradio ([License](https://github.com/gradio-app/gradio/blob/main/LICENSE)). Explore the chatbot gradio [implementation](https://huggingface.co/spaces/databricks-demos/chatbot/blob/main/app.py).\n",
    "\n",
    "*Note: this UI is hosted and maintained by Databricks for demo purpose and don't use the model you just created. We'll soon show you how to do that with Lakehouse Apps!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c9d3c0-5c84-4704-aef7-e37585592299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_gradio_app(\"databricks-demos-chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e524c98-bdbf-4cb4-b51f-a5b7fd4ca6ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your first GenAI RAG model!\n",
    "\n",
    "You're now ready to deploy the same logic for your internal knowledge base leveraging Lakehouse AI.\n",
    "\n",
    "We've seen how the Lakehouse AI is uniquely positioned to help you solve your GenAI challenge:\n",
    "\n",
    "- Simplify Data Ingestion and preparation with Databricks Engineering Capabilities\n",
    "- Accelerate Vector Search  deployment with fully managed indexes\n",
    "- Leverage Databricks LLama 2 foundation model endpoint\n",
    "- Deploy realtime model endpoint to perform RAG and provide Q&A capabilities\n",
    "\n",
    "Lakehouse AI is uniquely positioned to accelerate your GenAI deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39686c4-2895-4686-99c2-c6317a99097b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next: ready to take it to a next level?\n",
    "\n",
    "Open the [02-advanced/01-PDF-Advanced-Data-Preparation]($../02-advanced/01-PDF-Advanced-Data-Preparation) notebook series to learn more about unstructured data, advanced chain, model evaluation and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43398bab-08af-4546-88ac-b124b635c3b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "To free up resources, please delete uncomment and run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d12636-a76c-43f3-a1ec-748ecf40bff2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# /!\\ THIS WILL DROP YOUR DEMO SCHEMA ENTIRELY /!\\\n",
    "# cleanup_demo(catalog, db, serving_endpoint_name, f\"{catalog}.{db}.databricks_documentation_vs_index\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Deploy-RAG-Chatbot-Model-with-Arize-Phoenix",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
