{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOudyT6lPBqp"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
        "\n",
        "# <center>Getting Started with the Arize Platform</center>\n",
        "## <center>Investigating Embedding Drift in Image Classification</center>\n",
        "\n",
        "**In this walkthrough, we are going to ingest embedding data and look at embedding drift.** \n",
        "\n",
        "In this scenario, you are in charge of maintaining an Image Classification model. Your model, resnet-50, will classify the input images into the 10 predefined categories of the Fashion MNIST (see [dataset](https://huggingface.co/datasets/arize-ai/fashion_mnist_quality_drift)). However, once the model is released into production, you notice that the performance of the model has degraded over a period of time.\n",
        "\n",
        "\n",
        "This notebook will show you how Arize can automatically surface and troubleshoot the reason for this performance degradation by analyzing _image vectors_ associated with the input image so that you can take the right action to retrain your model/clean your data, saving you time and effort to correctly wrangle the datasets and visualize them. In this example, there are worse quality images in the production set during some period of time.\n",
        "\n",
        "It is worth noting that, according to our research, inspecting embedding drift can surface problems with your data before they cause performance degradation.\n",
        "\n",
        "In this tutorial, we will start from scratch. We will:\n",
        "* Download the data\n",
        "* Preprocess the data\n",
        "* Train the model\n",
        "* Extract image vectors and predictions\n",
        "* Log the inferences into the Arize Platform\n",
        "\n",
        "We will be using [ü§ó Hugging Face](https://huggingface.co/)'s open source libraries to make this process extremely easy. In particular, we will use:\n",
        "* [ü§ó Datasets](https://huggingface.co/docs/datasets/index): a library used for easily accessing and sharing datasets, and evaluation metrics for Computer Vision, Natural Language Processing (NLP), and audio tasks.\n",
        "* [ü§ó Transformers](https://huggingface.co/docs/transformers/index): a library used to easily download and use state-of-the-art pre-trained models. Using pre-trained models can lower your compute costs, reduce your carbon footprint, and save you time from training a model from scratch.\n",
        "\n",
        "Before we start, if this is your first Arize Tutorial, we recommend that you complete [Send Data to Arize in 5 Easy Steps](https://colab.research.google.com/github/Arize-ai/client_python/blob/main/arize/examples/tutorials/Arize_Tutorials/Quick_Start/Send_data_to_Arize_in_5_easy_steps_classification.ipynb) before continuing. If you are familiar with sending data to Arize, it only takes a few more lines to send embedding data. \n",
        "\n",
        "Let's get started! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X9GuXoSXleA"
      },
      "source": [
        "# Step 0. Setup and Getting the Data\n",
        "\n",
        "We will first install ü§óHugging Face's `datasets` and `transformers` libraries, mentioned above. In addition, we will import some metrics from `sklearn`. Find out more [here](https://github.com/scikit-learn/scikit-learn).\n",
        "\n",
        "We'll explain each of the imports below as we use them through this tutorial.\n",
        "\n",
        "\n",
        "## Install Dependencies and Import Libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvPo5LKZjpfs"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers arize umap-learn pandas==1.3.5 pickle5\n",
        "\n",
        "import tensorflow as tf \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, AutoFeatureExtractor\n",
        "from keras.preprocessing.image import image\n",
        "from transformers import ConvNextFeatureExtractor, ConvNextModel, ImageFeatureExtractionMixin\n",
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "from arize.pandas.logger import Client\n",
        "from arize.utils.types import Environments, ModelTypes, EmbeddingColumnNames, Schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mF98asfZvUf"
      },
      "source": [
        "## Check if GPU is available\n",
        "Here we use Pytorch to check whether a GPU is available or not. When appropriate, we will use PyTorch's `nn.Module.to()` method to ensure that the model will run on the GPU if we have one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef8TK-TxZswV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74GhmA4Adbe2"
      },
      "source": [
        "## **üåê Download the Data**\n",
        "\n",
        "The easiest way to load a dataset is from the [Hugging Face Hub](https://huggingface.co/datasets). The [arize-ai/fashion_mnist_quality_drift](https://huggingface.co/datasets/arize-ai/fashion_mnist_quality_drift) dataset has been crafted by Arize for this example notebook.\n",
        "\n",
        "Thanks to Hugging Face ü§ó Datasets, we can download the dataset in one line of code. The `Dataset` object comes equipped with methods that make it very easy to inspect, pre-process, and post-process your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNt3UFf8pwxD"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"arize-ai/fashion_mnist_quality_drift\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wehqNFNcMiuV"
      },
      "source": [
        "You can select the splits of the dataset as you would in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQI9Himx6P9R"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds, prod_ds = dataset['training'], dataset['validation'], dataset['production']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMkfZThnIsR3"
      },
      "source": [
        "## Inspect the Data\n",
        "\n",
        "It is often convenient to convert a `Dataset` object to a Pandas `DataFrame` so we can access high-level APIs for data visualization. ü§ó Datasets provides a `set_format()` method that allows us to change the output format of the `Dataset`. This does not change the underlying data format, an Arrow table. When the `DataFrame` format is no longer needed, we can reset the output format using `reset_format()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia5ARILmIrqB"
      },
      "outputs": [],
      "source": [
        "train_ds.set_format(\"pandas\")\n",
        "display(train_ds[:].head())\n",
        "train_ds.reset_format()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9RUh3yeIyUD"
      },
      "source": [
        "Let's also take a look at the categories we will be classifiying into:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVXvRoJI3eQ"
      },
      "outputs": [],
      "source": [
        "labels = train_ds.features[\"label\"]\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALcGD4v6I_bw"
      },
      "source": [
        "# Step 1. Setting up your Image Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41moQxywJEIJ"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "In order to input our data into our model for fine-tuning, we first need to perform some transformations: *convert to RGB* and *feature extraction*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert greyscale images to RGB\n",
        "\n",
        "We define the function `convert_to_rgb()` and we apply it to the entire dataset using the `map()` method which will convert all the images from greyscale to RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_rgb(batch):\n",
        "  return {'image': [image.convert(\"RGB\") for image in batch['image']]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_batch_size = 100\n",
        "train_ds = train_ds.map(convert_to_rgb, batched = True, batch_size = process_batch_size)\n",
        "val_ds = val_ds.map(convert_to_rgb, batched = True, batch_size = process_batch_size)\n",
        "prod_ds = prod_ds.map(convert_to_rgb, batched = True, batch_size = process_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI-7UzFxJD62"
      },
      "source": [
        "### Feature Extractor\n",
        "\n",
        "For audio and vision tasks, a feature extractor processes the audio signal or image into the correct input format. ü§ó Transformers provides the `AutoFeatureExtractor` class, which allows us to quickly download the FeatureExtractor required by the pre-trained model of our choosing. In this tutorial, we will use `microsoft/resnet-50`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a3vJn4ePC6B"
      },
      "outputs": [],
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\", do_resize = True)\n",
        "feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n",
        "\n",
        "With the feature extractor configuration above, we can now apply some transformations to augment our dataset and improve training results. In this case we chose transformations from the torchvision package: [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html?highlight=randomresizedcrop#torchvision.transforms.RandomResizedCrop) and [Normalize](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html?highlight=normalize#torchvision.transforms.Normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzS__gmIPIv6"
      },
      "source": [
        "### Image augmentation on the entire training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMoyqt8PkCS"
      },
      "source": [
        "Here we will use ü§ó Dataset‚Äôs [`with_transform()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?#datasets.Dataset.with_transform) method to apply the transforms over the entire dataset in batches. Since the transformations are meant to help training, we only apply them to the training and validation dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YbAzxI7RwP6"
      },
      "outputs": [],
      "source": [
        "def augmentation(dataset):\n",
        "    dataset[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in dataset[\"image\"]]\n",
        "    del dataset[\"image\"]      #deleting dataset[\"image\"] as the model only takes in \"pixel_values\" as inputs  \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfLIujP_RwNQ"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.with_transform(augmentation)\n",
        "val_ds = val_ds.with_transform(augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Eg8zYuSqST"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Similar to how we obtained the feature extractor, ü§ó Transformers provides the `AutoModelForImageClassification` class, which allows us to quickly download a pre-trained model with a token classification [task head](https://huggingface.co/course/en/chapter2/2?fw=pt#model-heads-making-sense-out-of-numbers) on top. The pre-trained model to use in this tutorial is [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50).\n",
        "\n",
        "It is important to pass `output_hidden_states = True` to be able to compute the embedding vectors associated with the image (explained below).\n",
        "\n",
        "_NOTE_: You can choose between fine-tuning the [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50) model in section A) or using a model that Arize has already fine-tuned for you in section B). To skip section A), set `SKIP_TRAINING = True` and go ahead to [_B) Download the model_](#B\\)-Download-the-fine-tuned-model).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZJQzbrnTYqU"
      },
      "outputs": [],
      "source": [
        "model_name = f\"microsoft/resnet-50\"\n",
        "SKIP_TRAINING = False # Make True if you want to skip training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSwCH1UMTRzA"
      },
      "source": [
        "### A) Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a6LJg9yTj19"
      },
      "source": [
        "Before downloading the pre-trained model, we will need to provide the mapping of each label to a label ID (integer) and vice versa to help the model recover the label name from the label ID.\n",
        "\n",
        "You will require Google Colab Pro or Pro+ to fine-tune the model in section A). If you want to avoid the cost, you can skip to section A) and go to [_B) Download the model_](#B\\)-Download-the-fine-tuned-model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8PQnzTsTi5j"
      },
      "outputs": [],
      "source": [
        "id2label = {idx: label for idx, label in enumerate(labels.names)}\n",
        "label2id = {label: idx for idx, label in enumerate(labels.names)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfA9fzi-TqOE"
      },
      "source": [
        "Let's download the pre-trained model.\n",
        "\n",
        "_NOTE_: You may get a warning from Hugging Face to `train this model on a down-stream task` when running the next cell, which can be ignored. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BwWN_MjR-9g"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=labels.num_classes,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    output_hidden_states=True,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZvSM-XhTuBC"
      },
      "source": [
        "Further, we use the [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments) class to define the training parameters. This class stores a lot of information and gives you control over the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4iHowNWR-68"
      },
      "outputs": [],
      "source": [
        "training_batch_size = 8\n",
        "training_epochs = 3\n",
        "logging_steps= len(train_ds) // training_batch_size\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    per_device_train_batch_size=training_batch_size,\n",
        "    per_device_eval_batch_size=training_batch_size,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=training_epochs,                        \n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps,\n",
        "    log_level=\"error\",\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=2e-4,\n",
        "    remove_unused_columns=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzPLX177UXmc"
      },
      "source": [
        "Now, we will define the evaluation function that calculates the accuracy and f1 score of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYfNpSFsUA6J"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions[0].argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-cy7L_SUttM"
      },
      "source": [
        "Next, we need a _data collator_ so that we can unpack and stack the batches that are coming in as lists of dicts into batch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrCBa9iFUOmO"
      },
      "outputs": [],
      "source": [
        "def collate_fn(dataset):\n",
        "    pixel_values = torch.stack([ds[\"pixel_values\"] for ds in dataset])\n",
        "    labels = torch.tensor([ds[\"label\"] for ds in dataset])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can fine-tune our model using the `Trainer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGzSPwWRT_LX"
      },
      "outputs": [],
      "source": [
        "if SKIP_TRAINING == False:\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      data_collator=collate_fn,\n",
        "      train_dataset=train_ds,\n",
        "      eval_dataset=val_ds,\n",
        "      tokenizer=feature_extractor,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "  print(\"Evaluation before training\")\n",
        "  eval = trainer.evaluate(eval_dataset=val_ds)\n",
        "  eval_df = pd.DataFrame({'Epoch':0, 'Validation Loss': eval['eval_loss'], 'Accuracy': eval['eval_accuracy'], 'F1': eval['eval_f1']}, index=[0])\n",
        "  display(eval_df)\n",
        "\n",
        "  torch.cuda.empty_cache() # Free up some memory\n",
        "\n",
        "  print(\"\\n\\nTraining...\")\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UVMYFbnl6Kg"
      },
      "source": [
        "### B) Download the fine-tuned model \n",
        "\n",
        "If you decided to skip section A), you can download the already fine-tuned model [arize-ai/resnet-50-fashion-mnist-quality-drift](https://huggingface.co/arize-ai/resnet-50-fashion-mnist-quality-drift) from Arize's page in the Hugging Face Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UziFBoeHagmP"
      },
      "outputs": [],
      "source": [
        "if SKIP_TRAINING == True: # Make sure you marked SKIP_TRAINING = True if you wanted to skip training\n",
        "    model_ckpt = f\"arize-ai/resnet-50-fashion-mnist-quality-drift\"\n",
        "\n",
        "    model = (AutoModelForImageClassification\n",
        "            .from_pretrained(model_ckpt, \n",
        "                             num_labels = labels.num_classes,\n",
        "                             output_hidden_states=True\n",
        "                             )\n",
        "            .to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7YBqdsebIF_"
      },
      "source": [
        "# Step 2. Post-Processing your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klUW0X57mVdA"
      },
      "source": [
        "## Get model outputs\n",
        "Now we will extract the prediction labels and the image embedding vectors. The latter are formed from the hidden states of our pre-trained (and then fine-tuned) model. We will choose the last hidden state layer, with a shape of `(batch_size, embedding_size, 7, 7)`*. To obtain the embedding vector, we will average on the last 2 dimensions.\n",
        "\n",
        "\n",
        "***NOTE:** The last 2 components of the shape (7, 7) are due to the output size of the last convolutional layer in the resnet-50 architecture. See Table 1 on page 5 in [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) for more information. In the same table, you can also see that the `embedding_size` is 2048."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TO3LXYtTMPg"
      },
      "outputs": [],
      "source": [
        "def postprocess(batch):\n",
        "    inputs = feature_extractor([x.convert(\"RGB\") for x in batch[\"image\"]], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    pred_labels = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "    last_hidden_states = outputs.hidden_states[-1]\n",
        "    embeddings= torch.mean(last_hidden_states, (2,3)).cpu().numpy()\n",
        "\n",
        "    return {'pred_label':pred_labels, 'image_vector': embeddings}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before applying the post-processing function defined above, we need to apply  [`reset_format()`](https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.reset_format) on the training and validation set in order to reset the dataset to their original formats that contained the `\"image\"` feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdHQmHpKhXjM"
      },
      "outputs": [],
      "source": [
        "train_ds.reset_format()\n",
        "val_ds.reset_format()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we apply the post-processing to all datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZtL2pNUmBf"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(postprocess, batched = True, batch_size = process_batch_size)\n",
        "val_ds = val_ds.map(postprocess, batched = True, batch_size = process_batch_size)\n",
        "prod_ds = prod_ds.map(postprocess, batched = True, batch_size = process_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BIeGAemfziv"
      },
      "source": [
        "# Step 3. Prepare your data to be sent to Arize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this point forward, it is convenient to use Pandas DataFrames. We can do so easily using the `to_pandas()` method that returns a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnOpfUVSMLeg"
      },
      "outputs": [],
      "source": [
        "train_df = train_ds.to_pandas()\n",
        "val_df = val_ds.to_pandas()\n",
        "prod_df = prod_ds.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyer0ywollCt"
      },
      "source": [
        "## Update the timestamps\n",
        "\n",
        "The data that you are working with was constructed in April of 2022. Hence, we will update the timestamps so they are current at the time that you're sending data to Arize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzYoV-hemYsE"
      },
      "outputs": [],
      "source": [
        "last_ts = max(prod_df['prediction_ts'])\n",
        "now_ts = datetime.timestamp(datetime.now())\n",
        "delta_ts = now_ts - last_ts    \n",
        "\n",
        "train_df['prediction_ts'] = (train_df['prediction_ts'] + delta_ts).astype(float)\n",
        "val_df['prediction_ts'] = (val_df['prediction_ts'] + delta_ts).astype(float)\n",
        "prod_df['prediction_ts'] = (prod_df['prediction_ts'] + delta_ts).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e37r7yiLpbvf"
      },
      "source": [
        "## Add prediction ids\n",
        "\n",
        "The Arize platform uses prediction IDs to link a prediction to an actual. Visit the [Arize documentation](https://docs.arize.com/arize/data-ingestion/model-schema/5.-prediction-id?q=prediction_id) for more details.\n",
        "\n",
        "You can generate prediction IDs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI4G0fNBm5UC"
      },
      "outputs": [],
      "source": [
        "def add_prediction_id(df):\n",
        "    return [str(uuid.uuid4()) for _ in range(df.shape[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n28vEY31niZ0"
      },
      "outputs": [],
      "source": [
        "train_df['prediction_id'] = add_prediction_id(train_df)\n",
        "val_df['prediction_id'] = add_prediction_id(val_df)\n",
        "prod_df['prediction_id'] = add_prediction_id(prod_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWLA7px4NmLb"
      },
      "source": [
        "## Convert integer labels to strings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3DvevhXNzvR"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df['label'].map(lambda label: id2label[label])\n",
        "train_df['pred_label'] = train_df['pred_label'].map(lambda label: id2label[label])\n",
        "\n",
        "val_df['label'] = val_df['label'].map(lambda label: id2label[label])\n",
        "val_df['pred_label'] = val_df['pred_label'].map(lambda label: id2label[label])\n",
        "\n",
        "prod_df['label'] = prod_df['label'].map(lambda label: id2label[label])\n",
        "prod_df['pred_label'] = prod_df['pred_label'].map(lambda label: id2label[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXrjo3Qpgm-"
      },
      "source": [
        "# Step 4. Sending Data into Arize üí´\n",
        "\n",
        "## Select the columns we want to send to Arize (optional)\n",
        "\n",
        "This step is not really necessary, since we will select the columns we want to send to Arize using the `Schema` definition (below). However, for the purpose of visibility, this is our final `DataFrame` with the data that will be sent to Arize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfi-z2sfmnyK"
      },
      "outputs": [],
      "source": [
        "arize_columns = [\n",
        "    'prediction_id', \n",
        "    'prediction_ts', \n",
        "    'label',\n",
        "    'pred_label',\n",
        "    'image_vector',\n",
        "    'url'\n",
        "    ]\n",
        "\n",
        "train_df = train_df[arize_columns]\n",
        "val_df = val_df[arize_columns]\n",
        "prod_df = prod_df[arize_columns]\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwvCUBqkeW4V"
      },
      "source": [
        "## Import and Setup Arize Client\n",
        "\n",
        "The first step is to setup the Arize client. After that we will log the data.\n",
        "\n",
        "Copy the Arize `API_KEY` and `SPACE_KEY` from your Space Settings page (shown below) to the variables in the cell below. We will also be setting up some metadata to use across all logging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3jd6J3qQb_K"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvkUY2dodBL5",
        "outputId": "e723839b-03b1-4c81-db3b-ea66693308a9"
      },
      "outputs": [],
      "source": [
        "SPACE_KEY = \"SPACE_KEY\"\n",
        "API_KEY = \"API_KEY\"\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "model_id = \"CV-demo-fashion-mnist-quality-drift\"\n",
        "model_version = \"1.0\"\n",
        "model_type = ModelTypes.SCORE_CATEGORICAL\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"‚úÖ Import and Setup Arize Client Done! Now we can start using Arize!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhYZ7xOVfasj"
      },
      "source": [
        "\n",
        "Now that our Arize client is set up, let's go ahead and log all of our data to the platform. For more details on how **`arize.pandas.logger`** works, visit our documentation.\n",
        "\n",
        "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCMZsMpi3aiA"
      },
      "source": [
        "## Define the Schema \n",
        "\n",
        "A Schema instance specifies the column names for corresponding data in the dataframe. While we could define different Schemas for training and production datasets, the dataframes have the same column names, so the Schema will be the same in this instance.\n",
        "\n",
        "To ingest non-embedding features, it suffices to provide a list of column names that contain the features in our dataframe. Embedding features, however, are a little bit different.\n",
        "\n",
        "Arize allows you to ingest not only the embedding vector but the raw data associated with that embedding, or a URL link to that raw data. Therefore, up to 3 columns can be associated with the same _embedding object_*. To be able to do this, Arize's SDK provides the `EmbeddingColumnNames` class, used below.\n",
        "\n",
        "\n",
        "***NOTE**: This is how we refer to the 3 possible pieces of information that can be sent as embedding objects:\n",
        "* Embedding `vector` (required)\n",
        "* Embedding `data` (optional): raw text associated with the embedding vector\n",
        "* Embedding `link_to_data` (optional): link to the data file (image, audio, ...) associated with the embedding vector\n",
        "\n",
        "\n",
        "\n",
        "Learn more [here](https://docs.arize.com/arize/data-ingestion/model-schema/7b.-embedding-features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOGpBqA034Zt"
      },
      "outputs": [],
      "source": [
        "features = []\n",
        "arize_columns = [\n",
        "    'prediction_id', \n",
        "    'prediction_ts', \n",
        "    'label',\n",
        "    'pred_label',\n",
        "    'image_vector',\n",
        "    'url'\n",
        "    ]\n",
        "\n",
        "\n",
        "embedding_features = [\n",
        "    EmbeddingColumnNames(\n",
        "        vector_column_name=\"image_vector\",  # Will be name of embedding feature in the app\n",
        "        link_to_data_column_name=\"url\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    timestamp_column_name=\"prediction_ts\",\n",
        "    prediction_label_column_name=\"pred_label\",\n",
        "    actual_label_column_name=\"label\",\n",
        "    feature_column_names=features,\n",
        "    embedding_feature_column_names=embedding_features\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw8vPvEj7sUu"
      },
      "source": [
        "## Log Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeevDQO_7sUu",
        "outputId": "5a678c46-2258-49de-d984-0c27b73a3459"
      },
      "outputs": [],
      "source": [
        "# Logging Training DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=train_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    model_type=model_type,\n",
        "    environment=Environments.TRAINING,\n",
        "    schema=schema,\n",
        "    sync=True\n",
        ")\n",
        "\n",
        "\n",
        "# If successful, the server will return a status_code of 200\n",
        "if response.status_code != 200:\n",
        "    print(f\"‚ùå logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged training set to Arize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dEt_DLx2bYl"
      },
      "source": [
        "## Log Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyUavdbiNfzy",
        "outputId": "ee596956-f992-42f3-c96c-4d1e0d3c8667"
      },
      "outputs": [],
      "source": [
        "# Logging Validation DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=val_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    batch_id=\"validation\",\n",
        "    model_type=model_type,\n",
        "    environment=Environments.VALIDATION,\n",
        "    schema=schema,\n",
        "    sync=True\n",
        ")\n",
        "\n",
        "\n",
        "# If successful, the server will return a status_code of 200\n",
        "if response.status_code != 200:\n",
        "    print(f\"‚ùå logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged training set to Arize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLeCjcQRDWF6"
      },
      "source": [
        "## Log Production Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXjzjXP2hsah",
        "outputId": "4793af5c-8941-4960-df88-964e6a81c215"
      },
      "outputs": [],
      "source": [
        "# Logging Production DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=prod_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    model_type=model_type,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=schema,\n",
        "    sync=True\n",
        ")\n",
        "\n",
        "if response.status_code != 200:\n",
        "    print(f\"‚ùå logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged production set to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX-GEkla5Rnh"
      },
      "source": [
        "# Step 5. Confirm Data in Arize ‚úÖ\n",
        "Note that the Arize platform takes about 15 minutes to index embedding data. While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to head over to the **Data Ingestion** tab for your model to watch Arize work its magic!üîÆ\n",
        "\n",
        "You will be able to see the predictions, actuals, and feature importances that have been sent in the last 30 minutes, last day or last week.\n",
        "\n",
        "An example view of the Data Ingestion tab from a model, when data is sent continuously over 30 minutes, is shown in the image below.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/data-ingestion-tab.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1JYzltL8Z3g"
      },
      "source": [
        "# Check the Embedding Data in Arize\n",
        "Now, you can see how Arize surfaces the low quality images before your customer does and troubleshoots the degradation in performance to save you the time and effort. \n",
        "\n",
        "First, set the baseline to the training set that we logged before.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/CV/fashion_mnist_baseline_setup.gif\" width=\"700\">\n",
        "\n",
        "\n",
        "Since your model contains embedding data, you will see it in your Model's Overview page. \n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/CV/fashion_mnist_embedding.png\" width=\"700\">\n",
        "\n",
        " Click on the Embedding Name or the Euclidean Distance value to see how your embedding data is drifting over time. In the picture below we represent the global euclidean distance between your production set (at different points in time) and the baseline (which we set to be our training set). We can see there is a period of a week where suddenly the distance is remarkably higher. This shows us that during that time image data was sent to our model that was different than what it was trained on. This is the period of time when the quality of some images is worse.\n",
        " \n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/CV/fashion_mnist_drift.png\" width=\"700\">\n",
        "\n",
        "In addition to the drift tracking plot above, below you can find the UMAP visualization of your data, according to the point in time selected. Notice that the production data and our baseline (training) data are superimposed, which is indicative that the model is seeing data in production similar to the data it was trained on.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/CV/fashion_mnist_no_drift_umap.png\" width=\"700\">\n",
        "\n",
        "For further inspection, you may select a 3D UMAP view and clicked _Explore UMAP_ to expand the view. With this view we can interact in 3D with our dataset. We can zoom, rotate, and drag so we can see the areas of our dataset that are most interesting to us. Check out the workflow below:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/CV/fashion-mnist-workflow.gif\" width=\"700\">\n",
        "\n",
        "In the display above, Arize offers many coloring options:\n",
        "1. By Dataset: You can see that the coloring has been made to distinguish production data vs baseline data (training in this example). This is specifically useful to detect drift. In this example, we can see that there is some production data far away from any training data, giving an indication of severe dataset drift. We can identify exactly what datapoints our baseline is missing so that we can re-train effectively.\n",
        "2. By Prediction Label: This coloring option gives an insight on how is our model making decisions. Where are the different classes located in the space? Is the model predicting one class in regions where it should be predicting another?\n",
        "3. By Actual Label: This coloring option is great if we want to identify labeling issues. For instance, if inside the orange cloud, we can see a points of other colors, it is a good idea to check and see if the labels are wrong. Further we can use the corrected labels for re-training. This separation is specially difficult when clusters are joined, since both the model and UMAP have trouble separating the data-points.\n",
        "4. By Correctness: This coloring option offers a quick way of identifying where the bulk of your model's mistakes are placed, giving you an area to pay attention to. In this example, we can see the difference between the red and blue images and almost all the red images have significantly worse quality (e.g. they are rotated and blurred).\n",
        "5. By Confusion Matrix: This coloring option allows you to select a `positive class` and color the data-points as `True Positives`, `True Negatives`, `False Positives`, `False Negatives`.\n",
        "\n",
        "More coloring options will be added to help you understand and debug your model and dataset, including coloring by feature values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aby-AFAe6jsV"
      },
      "source": [
        "# Wrap Up üéÅ\n",
        "Congratulations, you've now sent your first machine learning embedding data to the Arize platform!!\n",
        "\n",
        "Additionally, if you want to remove this example model from your account, just click **Models** -> **CV-demo-fashion-mnist-quality-drift** -> **config** -> **delete**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP304Ecf6ePX"
      },
      "source": [
        "### Overview\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Monitor Unstructured Data with Arize](https://arize.com/blog/monitor-unstructured-data-with-arize)\n",
        "- [Getting Started With Embeddings Is Easier Than You Think](https://arize.com/blog/getting-started-with-embeddings-is-easier-than-you-think)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "<!-- - [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/) -->\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "<!-- - [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/) -->\n",
        "\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "QOudyT6lPBqp"
      ],
      "machine_shape": "hm",
      "name": "Arize_Tutorial_HuggingFace_Image_Classification_Quality_Drift.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
