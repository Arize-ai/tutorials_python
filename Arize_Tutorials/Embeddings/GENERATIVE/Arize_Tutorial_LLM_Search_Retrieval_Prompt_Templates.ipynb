{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82de5c2",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "\n",
    "# <center>Getting Started with the Arize Platform</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709bb14",
   "metadata": {},
   "source": [
    "## <center>Prompt Engineering and Retrieval Workflows (LLM Observability)</center>\n",
    "This guide demonstrates how to use Arize for monitoring and debugging your LLM with retrieval augmented generation workflows and prompt engineering. We're going to use data from a chatbot built on top of Arize docs (https://docs.arize.com/arize/), with example query and retrieved text. Let's figure out how to understand how well our RAG system is working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf4144",
   "metadata": {},
   "source": [
    "# Step 0. Install Dependencies, Import Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe32db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7257001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from arize.pandas.logger import Client\n",
    "from arize.utils.types import (\n",
    "    Environments,\n",
    "    ModelTypes,\n",
    "    EmbeddingColumnNames,\n",
    "    Schema,\n",
    "    PromptTemplateColumnNames,\n",
    "    LLMConfigColumnNames,\n",
    "    LLMRunMetadataColumnNames,\n",
    "    CorpusSchema,\n",
    ")\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614da81",
   "metadata": {},
   "source": [
    "# Step 1. Download the data\n",
    "The data contains queries, retrieved context (from a corpus) used to augment generations, LLM responses and metdata. We're going to inspect this data further in the Arize platform, to understand the relationship between responses and corpus documents along with metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = (\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/\"\n",
    "    \"arize-demo-models-data/GENERATIVE/prompt-response/\"\n",
    ")\n",
    "prod_df = pd.read_parquet(data_url+\"df_queries_with_retrieved_doc_ids.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50894a1d",
   "metadata": {},
   "source": [
    "# Step 2. Prepare Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8ca60",
   "metadata": {},
   "source": [
    "## Add prediction ids\n",
    "\n",
    "The Arize platform uses prediction IDs to link a prediction to an actual. Visit the [Arize documentation](https://docs.arize.com/arize/data-ingestion/model-schema/5.-prediction-id?q=prediction_id) for more details.\n",
    "\n",
    "You can generate prediction IDs as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_id(df):\n",
    "    return [str(uuid.uuid4()) for _ in range(df.shape[0])]\n",
    "\n",
    "prod_df['prediction_id'] = add_prediction_id(prod_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acac800",
   "metadata": {},
   "source": [
    "## Update the timestamps\n",
    "\n",
    "The data that you are working with was constructed in August of 2023. Hence, we will update the timestamps so they are current at the time that you're sending data to Arize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ts = max(prod_df['prediction_ts'])\n",
    "now_ts = datetime.timestamp(datetime.now())\n",
    "delta_ts = now_ts - last_ts    \n",
    "\n",
    "prod_df['prediction_ts'] = (prod_df['prediction_ts'] + delta_ts).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b2ff1",
   "metadata": {},
   "source": [
    "# Step 3. Sending Data into Arize üí´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc5751",
   "metadata": {},
   "source": [
    "## Set up Arize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_KEY = \"usertest\"\n",
    "API_KEY = \"usertest\"\n",
    "\n",
    "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY, uri=\"https://devr.arize.com/v1\")\n",
    "model_id = \"search-and-retrieval-prompt-template-debug-demo\"\n",
    "model_version = \"1.0\"\n",
    "model_type = ModelTypes.GENERATIVE_LLM\n",
    "\n",
    "if SPACE_KEY == \"YOUR_SPACE_KEY\" or API_KEY == \"YOUR_API_KEY\":\n",
    "    raise ValueError(\"‚ùå CHANGE SPACE AND API KEYS\")\n",
    "else:\n",
    "    print(\"‚úÖ Arize client setup done! Now you can start using Arize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f843a",
   "metadata": {},
   "source": [
    "## Define the Schema \n",
    "\n",
    "A Schema instance specifies the column names for corresponding data in the dataframe. Arize is built with flexibility in mind - schema fields below are optional. The more data provided, the more targeted your debugging flows can be.\n",
    "\n",
    "For Prompt and response pairs, Arize allows you to ingest optional prompt and response values directly by providing `prompt_column_names` and `response_column_names` as fields of the Schema. Both prompt and response can be passed in as the following:\n",
    "- a single string column representing the raw text data column\n",
    "- (optional) as an embedding containing both an embedding and raw text associated with the embedding vector. Learn more about unstructured features [here](https://docs.arize.com/arize/sending-data/model-schema-reference#8.-embedding-features-unstructured).\n",
    "\n",
    "In addition, in this tutorial you will be sending information about your prompt templates, the LLM used and the hyper parameters used to configure it. Arize allows you to send this information by providing `prompt_template_column_names` and `llm_config_column_names`. We make use of the following classes:\n",
    "* `PromptTemplateColumnNames`: Groups together the prompt templates with their version\n",
    "    * `template_column_name`: Name of the column containing the promtp template in string format. The variables are represented by using the double key braces: `{{variable_name}}`.\n",
    "    * `template_version_column_name`: Name of column containing the version of the template used. This will allow you to filter by this field in the Arize platform.\n",
    "* `LLMConfigColumnNames`: Groups together the LLM used and the hyper parameters passed to it.\n",
    "    * `model_column_name`: Name of the column containing the names of the LLMs used to produce responses to the prompts. Typical examples are \"gpt-3.5turbo\" or `gpt-4\".\n",
    "    * `params_column_name`: Name of column containing the hyperparameters used to configure the LLM used. The contents of the column must be well formatted JSON string. For example: `{'max_tokens': 500, 'presence_penalty': 0.66, 'temperature': 0.28}`\n",
    "\n",
    "Learn more about Arize's prompt engineering workflows [here](https://docs.arize.com/arize/llm-large-language-models/prompt-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare prompt and response columns\n",
    "prompt_columns=EmbeddingColumnNames(\n",
    "    vector_column_name=\"prompt_vector\",\n",
    "    data_column_name=\"prompt_text\"\n",
    ")\n",
    "\n",
    "response_columns=\"response_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the columns for the prompt template playground\n",
    "prompt_template_columns = PromptTemplateColumnNames(\n",
    "        template_column_name=\"prompt_template\",\n",
    "        template_version_column_name=\"prompt_template_name\"\n",
    ")\n",
    "llm_config_columns = LLMConfigColumnNames(\n",
    "        model_column_name=\"llm_config_model_name\",\n",
    "        params_column_name=\"llm_params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5d153",
   "metadata": {},
   "source": [
    "We will also send metadata associated with your LLM activity. Sending in this data allows you to immediately utilize Arize's default dashboard for search and retrieval generative LLM models. Once your data has been sent into Arize, navigate to the Dashboard tab and click on the \"Generate Default Dasbhoard\" to create your dashboard.\n",
    "\n",
    "* `LLMRunMetadataColumnNames`: Groups together LLM run metadata\n",
    "    * `prompt_token_count_column_name`: Name of the column containing the names of the LLMs used to produce responses to the prompts. Typical examples are \"gpt-3.5turbo\" or `gpt-4\".\n",
    "    * `response_token_count_column_name`: Name of column containing the hyperparameters used to configure the LLM used. The contents of the column must be well formatted JSON string. For example: `{'max_tokens': 500, 'presence_penalty': 0.66, 'temperature': 0.28}`\n",
    "    * `total_token_count_column_name`: Name of column containing the hyperparameters used to configure the LLM used. The contents of the column must be well formatted JSON string. For example: `{'max_tokens': 500, 'presence_penalty': 0.66, 'temperature': 0.28}`\n",
    "    * `response_latency_ms_column_name`: Name of column containing the hyperparameters used to configure the LLM used. The contents of the column must be well formatted JSON string. For example: `{'max_tokens': 500, 'presence_penalty': 0.66, 'temperature': 0.28}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run_metadata_columns = LLMRunMetadataColumnNames(\n",
    "  total_token_count_column_name=\"total_token_count\",\n",
    "  prompt_token_count_column_name=\"prompt_token_count\",\n",
    "  response_token_count_column_name=\"response_token_count\",\n",
    "  response_latency_ms_column_name=\"response_latency_ms\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5e96e",
   "metadata": {},
   "source": [
    "Now, let's finalize the Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a465590",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_columns = [\n",
    "    \"cost_per_call\",\n",
    "    \"euclidean_distance_0\",\n",
    "    \"euclidean_distance_1\",\n",
    "    \"instruction\",\n",
    "    \"openai_precision_1\",\n",
    "    \"openai_precision_2\",\n",
    "    \"openai_relevance_0\",\n",
    "    \"openai_relevance_1\",\n",
    "    \"prompt_template\",\n",
    "    \"prompt_template_name\",\n",
    "    \"retrieval_text_0\",\n",
    "    \"retrieval_text_1\",\n",
    "    \"text_similarity_0\",\n",
    "    \"text_similarity_1\",\n",
    "    \"user_query\",\n",
    "    \"is_hallucination\",\n",
    "    \"llm_config_model_name\",\n",
    "]\n",
    "\n",
    "prod_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"pred_label\",\n",
    "    actual_label_column_name=\"user_feedback\",\n",
    "    tag_column_names=tag_columns,\n",
    "    prompt_column_names=prompt_columns,\n",
    "    response_column_names=response_columns,\n",
    "    prompt_template_column_names=prompt_template_columns,\n",
    "    llm_config_column_names=llm_config_columns,\n",
    "    llm_run_metadata_column_names=llm_run_metadata_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49202924",
   "metadata": {},
   "source": [
    "## Send Production Data\n",
    "Using the production dataset dataframe we prepared and the Schema we just defined, send the data into Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet files do not support maps with list and non-list values, so they are instead stored as valid json strings. \n",
    "# Convert these llm params that are stored as valid json strings into python dictionaries. \n",
    "prod_df[\"llm_params\"] = prod_df[\"llm_params\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "response = arize_client.log(\n",
    "    dataframe=prod_df,\n",
    "    schema=prod_schema,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=model_type,\n",
    "    environment=Environments.PRODUCTION\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Successfully logged data for model {model_id} to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'‚ùå Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0454b7",
   "metadata": {},
   "source": [
    "## Send Corpus Data\n",
    "The \"corpus\" dataset contains references to retrieved \"documents\" which were used to augment our LLM's response. First, let's download the corpus dataset and take a quick peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_parquet(data_url+\"df_corpus_docs.parquet\")\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a6d4",
   "metadata": {},
   "source": [
    "Next, define the schema required for sending in corpus data. The following fields will be needed:\n",
    "*   `document_id_column_name` - This maps to the column in the corpus dataframe containing the IDs that will be referenced by the `retrieved_document_ids` from the production dataframe.\n",
    "*   `document_text_embedding_column_names` - The embedding column names for the Corpus document\n",
    "*   `document_version_column_name` - The column name for the document version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa79c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_schema=CorpusSchema(\n",
    "    document_id_column_name='document_id',\n",
    "    document_text_embedding_column_names=EmbeddingColumnNames(\n",
    "        vector_column_name='text_vector',\n",
    "        data_column_name='text',\n",
    "    ),\n",
    "    document_version_column_name='document_version'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f6c47",
   "metadata": {},
   "source": [
    "Now, let's send in the corpus data into Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = arize_client.log(\n",
    "    dataframe=corpus_df,\n",
    "    schema=corpus_schema,\n",
    "    model_id=model_id,\n",
    "    model_type=model_type,\n",
    "    model_version=model_version,\n",
    "    environment=Environments.CORPUS,\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Successfully logged data for model {model_id} to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'‚ùå Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
