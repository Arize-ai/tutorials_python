{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82de5c2",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "\n",
    "# <center>Getting Started with the Arize Platform</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709bb14",
   "metadata": {},
   "source": [
    "## <center>Prompt Engineering and Retrieval Workflows (LLM Observability) With Token Generation</center>\n",
    "This guide demonstrates how to use Arize for monitoring and debugging your LLM with retrieval augmented generation workflows and prompt engineering, as well as how to generate token count information that is important for managing your LLM usage. We're going to use data from a chatbot built on top of Arize docs (https://docs.arize.com/arize/), with example query and retrieved text. Let's figure out how to understand how well our RAG system is working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf4144",
   "metadata": {},
   "source": [
    "# Step 0. Install Dependencies, Import Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe32db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7257001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from arize.pandas.logger import Client\n",
    "from arize.utils.types import (\n",
    "    Environments,\n",
    "    ModelTypes,\n",
    "    EmbeddingColumnNames,\n",
    "    Schema,\n",
    "    PromptTemplateColumnNames,\n",
    "    LLMConfigColumnNames,\n",
    "    LLMRunMetadataColumnNames,\n",
    "    CorpusSchema,\n",
    ")\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614da81",
   "metadata": {},
   "source": [
    "# Step 1. Download the data\n",
    "The data contains queries, retrieved context (from a corpus) used to augment generations, LLM responses and metdata. We're going to inspect this data further in the Arize platform, to understand the relationship between responses and corpus documents along with metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = (\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/\"\n",
    "    \"arize-demo-models-data/GENERATIVE/prompt-response/\"\n",
    ")\n",
    "prod_df = pd.read_parquet(\n",
    "    data_url + \"df_queries_for_token_count_generation.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50894a1d",
   "metadata": {},
   "source": [
    "# Step 2. Prepare Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8ca60",
   "metadata": {},
   "source": [
    "## Add prediction ids\n",
    "\n",
    "The Arize platform uses prediction IDs to link a prediction to an actual. Visit the [Arize documentation](https://docs.arize.com/arize/data-ingestion/model-schema/5.-prediction-id?q=prediction_id) for more details.\n",
    "\n",
    "You can generate prediction IDs as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_id(df):\n",
    "    return [str(uuid.uuid4()) for _ in range(df.shape[0])]\n",
    "\n",
    "\n",
    "prod_df[\"prediction_id\"] = add_prediction_id(prod_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acac800",
   "metadata": {},
   "source": [
    "## Update the timestamps\n",
    "\n",
    "The data that you are working with was constructed in August of 2023. Hence, we will update the timestamps so they are current at the time that you're sending data to Arize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ts = max(prod_df[\"prediction_ts\"])\n",
    "now_ts = datetime.timestamp(datetime.now())\n",
    "delta_ts = now_ts - last_ts\n",
    "\n",
    "prod_df[\"prediction_ts\"] = (prod_df[\"prediction_ts\"] + delta_ts).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5b76f",
   "metadata": {},
   "source": [
    "# Step 3. Generate Token Counts\n",
    "Arize supports tracking fields that designate LLM token usage as part of the Arize schema. Token counts are important for understand how efficiently you are using LLMs and the total cost you're incurring from your usage. Defining these fields in Arize will allow you to create a default LLM dashboard from the Dashboard view. \n",
    "\n",
    "To generate token counts, we will be using [tiktoken](https://github.com/openai/tiktoken), a byte-pair encoding tokeniser library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "encoding_name = \"cl100k_base\"  # or any another encoding you want to use\n",
    "\n",
    "prod_df[\"prompt_token_count\"] = prod_df[\"prompt_text\"].apply(\n",
    "    lambda x: num_tokens_from_string(x, encoding_name)\n",
    ")\n",
    "prod_df[\"response_token_count\"] = prod_df[\"response_text\"].apply(\n",
    "    lambda x: num_tokens_from_string(x, encoding_name)\n",
    ")\n",
    "prod_df[\"total_token_count\"] = (\n",
    "    prod_df[\"response_token_count\"] + prod_df[\"prompt_token_count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b2ff1",
   "metadata": {},
   "source": [
    "# Step 4. Sending Data into Arize üí´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc5751",
   "metadata": {},
   "source": [
    "## Set up Arize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_ID = \"SPACE_ID\"\n",
    "API_KEY = \"API_KEY\"\n",
    "\n",
    "arize_client = Client(space_id=SPACE_ID, api_key=API_KEY)\n",
    "model_id = \"search-and-retrieval-prompt-template-debug-with-token-counts-demo\"\n",
    "model_version = \"1.0\"\n",
    "model_type = ModelTypes.GENERATIVE_LLM\n",
    "\n",
    "if SPACE_ID == \"SPACE_ID\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"‚ùå CHANGE SPACE_ID AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"‚úÖ Arize client setup done! Now you can start using Arize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f843a",
   "metadata": {},
   "source": [
    "## Define the Schema \n",
    "\n",
    "A Schema instance specifies the column names for corresponding data in the dataframe. Arize is built with flexibility in mind - the LLM Schema fields below are optional. The more data provided, the more targeted your debugging flows can be. Learn more about defining Schemas for LLM data [here](https://docs.arize.com/arize/model-types/large-language-models-llm).\n",
    "\n",
    "For Prompt and response pairs, Arize allows you to ingest optional prompt and response values directly by providing `prompt_column_names` and `response_column_names` as fields of the Schema. Both prompt and response can be passed in as the following:\n",
    "- a single string column representing the raw text data column\n",
    "- (optional) as an embedding containing both an embedding and raw text associated with the embedding vector. Learn more about unstructured features [here](https://docs.arize.com/arize/sending-data/model-schema-reference#8.-embedding-features-unstructured).\n",
    "\n",
    "In addition, in this tutorial you will be sending information about your prompt templates, the LLM used and the hyper parameters used to configure it. Arize allows you to send this information by providing `prompt_template_column_names` and `llm_config_column_names`. We make use of the following classes:\n",
    "* `PromptTemplateColumnNames` (optional):  Groups together the prompt templates with their version\n",
    "    * `template_column_name`: Name of the column containing the promtp template in string format. The variables are represented by using the double key braces: `{{variable_name}}`.\n",
    "    * `template_version_column_name`: Name of column containing the version of the template used. This will allow you to filter by this field in the Arize platform.\n",
    "* `LLMConfigColumnNames` (optional): Groups together the LLM used and the hyper parameters passed to it.\n",
    "    * `model_column_name`: Name of the column containing the names of the LLMs used to produce responses to the prompts. Typical examples are \"gpt-3.5turbo\" or `gpt-4\".\n",
    "    * `params_column_name`: Name of column containing the hyperparameters used to configure the LLM used. The contents of the column must be well formatted JSON string. For example: `{'max_tokens': 500, 'presence_penalty': 0.66, 'temperature': 0.28}`\n",
    "\n",
    "Learn more about Arize's prompt engineering workflows [here](https://docs.arize.com/arize/llm-large-language-models/prompt-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare prompt and response columns\n",
    "prompt_columns = EmbeddingColumnNames(\n",
    "    vector_column_name=\"prompt_vector\", data_column_name=\"prompt_text\"\n",
    ")\n",
    "\n",
    "response_columns = \"response_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the columns for the prompt template playground\n",
    "prompt_template_columns = PromptTemplateColumnNames(\n",
    "    template_column_name=\"prompt_template\",\n",
    "    template_version_column_name=\"prompt_template_name\",\n",
    ")\n",
    "llm_config_columns = LLMConfigColumnNames(\n",
    "    model_column_name=\"llm_config_model_name\",\n",
    "    params_column_name=\"llm_params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5d153",
   "metadata": {},
   "source": [
    "Now we will use the token counts we generated above in the Schema, along with the LLM response latency data in our data, and we will send that data into Arize.\n",
    "\n",
    "* `LLMRunMetadataColumnNames` (optional): Groups together LLM run metadata\n",
    "    * `prompt_token_count_column_name`: Name of column containing the number of tokens used in the prompt that is sent to the LLM.\n",
    "    * `response_token_count_column_name`: Name of column containing he number of tokens used in the LLM's response. \n",
    "    * `total_token_count_column_name`: Name of column containing the number of tokens used between both the prompt and response tokens.\n",
    "    * `response_latency_ms_column_name`: Name of column containing the time it took for the LLM to respond (latency) in milliseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run_metadata_columns = LLMRunMetadataColumnNames(\n",
    "    total_token_count_column_name=\"total_token_count\",\n",
    "    prompt_token_count_column_name=\"prompt_token_count\",\n",
    "    response_token_count_column_name=\"response_token_count\",\n",
    "    response_latency_ms_column_name=\"response_latency_ms\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5e96e",
   "metadata": {},
   "source": [
    "Now, let's finalize the Schema. Learn more about the other Schema fields [here](https://docs.arize.com/arize/sending-data-guides/model-schema-reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a465590",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_columns = [\n",
    "    \"cost_per_call\",\n",
    "    \"euclidean_distance_0\",\n",
    "    \"euclidean_distance_1\",\n",
    "    \"instruction\",\n",
    "    \"openai_precision_1\",\n",
    "    \"openai_precision_2\",\n",
    "    \"openai_relevance_0\",\n",
    "    \"openai_relevance_1\",\n",
    "    \"prompt_template\",\n",
    "    \"prompt_template_name\",\n",
    "    \"retrieval_text_0\",\n",
    "    \"retrieval_text_1\",\n",
    "    \"text_similarity_0\",\n",
    "    \"text_similarity_1\",\n",
    "    \"user_query\",\n",
    "    \"is_hallucination\",\n",
    "    \"llm_config_model_name\",\n",
    "]\n",
    "\n",
    "prod_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"pred_label\",\n",
    "    actual_label_column_name=\"user_feedback\",\n",
    "    tag_column_names=tag_columns,\n",
    "    prompt_column_names=prompt_columns,\n",
    "    response_column_names=response_columns,\n",
    "    prompt_template_column_names=prompt_template_columns,\n",
    "    llm_config_column_names=llm_config_columns,\n",
    "    llm_run_metadata_column_names=llm_run_metadata_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49202924",
   "metadata": {},
   "source": [
    "## Send Production Data\n",
    "Using the production dataset dataframe we prepared and the Schema we just defined, send the data into Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet files do not support maps with list and non-list values, so they are instead stored as valid json strings.\n",
    "# Convert these llm params that are stored as valid json strings into python dictionaries.\n",
    "prod_df[\"llm_params\"] = prod_df[\"llm_params\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "response = arize_client.log(\n",
    "    dataframe=prod_df,\n",
    "    schema=prod_schema,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=model_type,\n",
    "    environment=Environments.PRODUCTION,\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Successfully logged data for model {model_id} to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'‚ùå Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0454b7",
   "metadata": {},
   "source": [
    "## Send Corpus Data\n",
    "The \"corpus\" dataset contains references to retrieved \"documents\" which were used to augment our LLM's response. First, let's download the corpus dataset and take a quick peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_parquet(data_url + \"df_corpus_docs.parquet\")\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a6d4",
   "metadata": {},
   "source": [
    "Next, define the Schema required for sending in corpus data. The following fields will be needed:\n",
    "*   `document_id_column_name` - This maps to the column in the corpus dataframe containing the IDs that will be referenced by the `retrieved_document_ids` from the production dataframe.\n",
    "*   `document_text_embedding_column_names` - The embedding column names for the Corpus document\n",
    "*   `document_version_column_name` - The column name for the document version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa79c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_schema = CorpusSchema(\n",
    "    document_id_column_name=\"document_id\",\n",
    "    document_text_embedding_column_names=EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",\n",
    "        data_column_name=\"text\",\n",
    "    ),\n",
    "    document_version_column_name=\"document_version\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f6c47",
   "metadata": {},
   "source": [
    "Now, let's send in the corpus data into Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = arize_client.log(\n",
    "    dataframe=corpus_df,\n",
    "    schema=corpus_schema,\n",
    "    model_id=model_id,\n",
    "    model_type=model_type,\n",
    "    model_version=model_version,\n",
    "    environment=Environments.CORPUS,\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"‚úÖ Successfully logged data for model {model_id} to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'‚ùå Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daa7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
