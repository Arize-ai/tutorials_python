{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nScIcEuJi4MH"
   },
   "source": [
    "Evaluating and Improving a LlamaIndex Semantic Retrieval Application\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LlamaIndex. Users send questions about Arize's core product via a chat interface, and your service retrieves documents from your documentation in order to generate a response to the user. As the engineer in charge of evaluating and maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Arize helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant context\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base of the Arize documentation\n",
    "- Visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using cosine similarity, Euclidean distance, and LLM-assisted ranking metrics\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCos8vzci4MJ"
   },
   "source": [
    "## 1. Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhtUwl70i4MJ"
   },
   "source": [
    "Install Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4cle-iQi4MK"
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRfWM3jhi4ML"
   },
   "source": [
    "## 2. Download Your Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqqfGNbci4ML"
   },
   "source": [
    "Download and unzip a pre-built knowledge base index consisting of chunks of the Arize documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xirqS78_i4MN"
   },
   "source": [
    "## 3. Load Your Data Into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_index_database_into_dataframe(docstore, vector_store) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads LlamaIndex data into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    embeddings_list = []\n",
    "    for doc_id in docstore[\"docstore/data\"]:\n",
    "        text_list.append(docstore[\"docstore/data\"][doc_id][\"__data__\"][\"text\"])\n",
    "        embeddings_list.append(np.array(vector_store[\"embedding_dict\"][doc_id]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": text_list,\n",
    "            \"text_vector\": embeddings_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(os.path.join(index_dir, \"docstore.json\")) as f:\n",
    "    docstore = json.load(f)\n",
    "with open(os.path.join(index_dir, \"vector_store.json\")) as f:\n",
    "    vector_store = json.load(f)\n",
    "\n",
    "database_df = load_llama_index_database_into_dataframe(docstore, vector_store).drop_duplicates(\n",
    "    subset=[\"text\"]\n",
    ")\n",
    "database_df['pred'] = 0\n",
    "database_df['pred_id'] = database_df.index\n",
    "\n",
    "database_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieJYE9eKi4MN"
   },
   "source": [
    "The columns of your dataframe are:\n",
    "- **text:** the chunked text in your knowledge base\n",
    "- **text_vector:** the embedding vector for the text, computed during the LlamaIndex build using \"text-embedding-ada-002\" from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM47th6Zi4MO"
   },
   "source": [
    "Next, download a dataframe containing query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = (\n",
    "    pd.read_parquet(\n",
    "        \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals_with_user_feedback.parquet\"\n",
    "    )\n",
    "    .rename(columns={\"query_text\": \"text\", \"query_embedding\": \"text_vector\"})\n",
    "    .drop(columns=[\"context_doc_id_0\", \"context_doc_id_1\"])\n",
    ")\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPbI3YZki4MO"
   },
   "source": [
    "The columns of the dataframe are:\n",
    "- **text:** the query text\n",
    "- **text_vector:** the embedding representation of the query, captured from LlamaIndex at query time\n",
    "- **response:** the final response from the LlamaIndex application\n",
    "- **context_text_0:** the first retrieved context from the knowledge base\n",
    "- **context_similarity_0:** the cosine similarity between the query and the first retrieved context\n",
    "- **context_text_1:** the second retrieved context from the knowledge base\n",
    "- **context_similarity_1:** the cosine similarity between the query and the first retrieved context\n",
    "- **user_feedback:** approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aREeSVgai4MO"
   },
   "source": [
    "The query and database datasets are drawn from different distributions; the queries are short questions while the database entries are several sentences to a paragraph. The embeddings from OpenAI's \"text-embedding-ada-002\" capture these differences and naturally separate the query and context embeddings into distinct regions of the embedding space. When using Arize, you want to \"overlay\" the query and context embedding distributions so that queries appear close to their retrieved context in the Arize point cloud. To achieve this, we compute a centroid for each dataset that represents an average point in the embedding distribution and center the two distributions so they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_centroid = database_df[\"text_vector\"].mean()\n",
    "database_df[\"centered_text_vector\"] = database_df[\"text_vector\"].apply(\n",
    "    lambda x: x - database_centroid\n",
    ")\n",
    "query_centroid = query_df[\"text_vector\"].mean()\n",
    "query_df[\"centered_text_vector\"] = query_df[\"text_vector\"].apply(lambda x: x - query_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7URqZE5gi4MP"
   },
   "source": [
    "## 4. Compute Proxy Metrics for Retrieval Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0N3fBh-i4MP"
   },
   "source": [
    "Cosine similarity and Euclidean distance can act as proxies for retrieval quality. The cosine distance between query and retrieved context was computed at query time and is part of the query dataframe downloaded above. Compute the Euclidean distance between each query embedding and retrieved context embedding and add corresponding columns to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_distance(\n",
    "    vector0: npt.NDArray[np.float32], vector1: npt.NDArray[np.float32]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector0 - vector1)\n",
    "\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    euclidean_distances = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        query_embedding = row[\"text_vector\"]\n",
    "        context_text = row[f\"context_text_{context_index}\"]\n",
    "        database_row = database_df[database_df[\"text\"] == context_text].iloc[0]\n",
    "        database_embedding = database_row[\"text_vector\"]\n",
    "        euclidean_distance = compute_euclidean_distance(query_embedding, database_embedding)\n",
    "        euclidean_distances.append(euclidean_distance)\n",
    "    query_df[f\"euclidean_distance_{context_index}\"] = euclidean_distances\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XmWO-qki4MP"
   },
   "source": [
    "## 5. Run LLM-Assisted Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO_U099Ri4MP"
   },
   "source": [
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to measure retrieval quality by simply asking the LLM whether each piece of context is relevant to the corresponding query.\n",
    "\n",
    "Use OpenAI to predict whether each retrieved document is relevant or irrelevant to the query. Running evaluations across the entire dataset takes a while, so for this example we're downloading a dataset of pre-computed evaluations and adding to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_evaluations_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/evaluations.parquet\"\n",
    ")[[\"text\", \"relevance_0\", \"relevance_1\"]]\n",
    "openai_evaluations_df = openai_evaluations_df.rename(\n",
    "    columns={\"relevance_0\": \"openai_relevance_0\", \"relevance_1\": \"openai_relevance_1\"}\n",
    ")\n",
    "query_df = pd.merge(query_df, openai_evaluations_df, on=\"text\")\n",
    "query_df[[\"text\", \"context_text_0\", \"context_text_1\", \"openai_relevance_0\", \"openai_relevance_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKuKN_Rli4MQ"
   },
   "source": [
    "For comparison, we've also run evaluations using Google PaLM 2. Download those pre-computed evaluations and add to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm2_evaluations_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/palm2_evaluations.parquet\"\n",
    ")[[\"text\", \"relevance_0\", \"relevance_1\"]]\n",
    "palm2_evaluations_df = palm2_evaluations_df.rename(\n",
    "    columns={\"relevance_0\": \"palm2_relevance_0\", \"relevance_1\": \"palm2_relevance_1\"}\n",
    ")\n",
    "query_df = pd.merge(query_df, palm2_evaluations_df, on=\"text\")\n",
    "query_df[[\"text\", \"context_text_0\", \"context_text_1\", \"palm2_relevance_0\", \"palm2_relevance_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n53WwTVri4MQ"
   },
   "source": [
    "Check the percent of agreeing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_equal = 0\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    equal_relevance_mask = (\n",
    "        query_df[f\"openai_relevance_{context_index}\"]\n",
    "        == query_df[f\"palm2_relevance_{context_index}\"]\n",
    "    )\n",
    "    num_equal += equal_relevance_mask.sum()\n",
    "percent_agreeing = num_equal / (len(query_df) * num_retrieved_documents)\n",
    "percent_agreeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PbD_hjPi4MU"
   },
   "source": [
    "You can see that for the vast majority of cases, the two LLMs agree. View the few examples where they disagree in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals_df = pd.concat(\n",
    "    [\n",
    "        query_df[\n",
    "            [\n",
    "                \"text\",\n",
    "                f\"context_text_{context_index}\",\n",
    "                f\"openai_relevance_{context_index}\",\n",
    "                f\"palm2_relevance_{context_index}\",\n",
    "            ]\n",
    "        ].rename(\n",
    "            columns={\n",
    "                f\"context_text_{context_index}\": \"context_text\",\n",
    "                f\"openai_relevance_{context_index}\": \"openai_relevance\",\n",
    "                f\"palm2_relevance_{context_index}\": \"palm2_relevance\",\n",
    "            }\n",
    "        )\n",
    "        for context_index in range(num_retrieved_documents)\n",
    "    ]\n",
    ")\n",
    "disagreeing_evaluation_mask = retrievals_df[\"openai_relevance\"] != retrievals_df[\"palm2_relevance\"]\n",
    "retrievals_df[disagreeing_evaluation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dn2AbA2i4MU"
   },
   "source": [
    "## 6. Compute Ranking Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNeVyRrLi4MV"
   },
   "source": [
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for k = 1, 2 for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "precision@k = (# of top-k retrieved documents that are relevant) / (k retrieved documents)\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LlamaIndex application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved.\n",
    "\n",
    "Compute precision@k for k = 1, 2 and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"openai\", \"palm2\"]:\n",
    "    num_relevant_documents_array = np.zeros(len(query_df))\n",
    "    for retrieved_context_index in range(0, num_retrieved_documents):\n",
    "        num_retrieved_documents = retrieved_context_index + 1\n",
    "        num_relevant_documents_array += (\n",
    "            query_df[f\"{model_name}_relevance_{retrieved_context_index}\"]\n",
    "            .map(lambda x: int(x == \"relevant\"))\n",
    "            .to_numpy()\n",
    "        )\n",
    "        query_df[f\"{model_name}_precision@{num_retrieved_documents}\"] = pd.Series(\n",
    "            num_relevant_documents_array / num_retrieved_documents\n",
    "        )\n",
    "\n",
    "query_df[\n",
    "    [\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "        \"palm2_relevance_0\",\n",
    "        \"palm2_relevance_1\",\n",
    "        \"palm2_precision@1\",\n",
    "        \"palm2_precision@2\",\n",
    "    ]\n",
    "]\n",
    "query_df[\"response_vector\"] = query_df[\n",
    "    \"centered_text_vector\"\n",
    "].copy()\n",
    "\n",
    "query_df['context_text_1'] = query_df['context_text_1'].apply(lambda x: x[:1000])\n",
    "query_df['context_text_0'] = query_df['context_text_0'].apply(lambda x: x[:1000])\n",
    "query_df.rename(columns={\"openai_precision@1\":\"openai_precision_1\",\n",
    "        \"openai_precision@2\":\"openai_precision_2\",\n",
    "        \"palm2_precision@1\":\"palm2_precision_1\",\n",
    "        \"palm2_precision@2\":\"palm2_precision_2\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSzxc_2ewzLH"
   },
   "source": [
    "## 7. Send the data to Arize for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sign up/ log in to your Arize account [here](https://app.arize.com/auth/login). Find your [space and API keys](https://docs.arize.com/arize/api-reference/arize.pandas/client). Copy/paste into the cell below. For more information on Arize spaces, please visit the [Authentication section](https://docs.arize.com/arize/api-reference/rest-api#authentication) of the Arize AI docs.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.pandas.logger import Client, Schema\n",
    "from arize.utils.types import ModelTypes, Environments, Schema, EmbeddingColumnNames\n",
    "\n",
    "SPACE_KEY = \"SPACE_KEY\"  # Change this line\n",
    "API_KEY = \"API_KEY\"  # Change this line\n",
    "MODEL_ID = 'arize-tutorial-search-and-retrieval'  # Change this line\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"✅ Import and Setup Arize Client Done! Now we can start using Arize!\")\n",
    "\n",
    "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
    "\n",
    "\n",
    "prod_schema = Schema(\n",
    "    prompt_column_names=EmbeddingColumnNames(\n",
    "        data_column_name=\"text\",\n",
    "        vector_column_name=\"centered_text_vector\",\n",
    "    ),\n",
    "    response_column_names=EmbeddingColumnNames(\n",
    "        data_column_name=\"response\",\n",
    "        vector_column_name=\"response_vector\",\n",
    "    ),\n",
    "    tag_column_names=[\n",
    "        \"context_text_0\",\n",
    "        \"context_similarity_0\",\n",
    "        \"context_text_1\",\n",
    "        \"context_similarity_1\",\n",
    "        \"euclidean_distance_0\",\n",
    "        \"euclidean_distance_1\",\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"palm2_relevance_0\",\n",
    "        \"palm2_relevance_1\",\n",
    "        \"openai_precision_1\",\n",
    "        \"openai_precision_2\",\n",
    "        \"palm2_precision_1\",\n",
    "        \"palm2_precision_2\",\n",
    "        \"user_feedback\",\n",
    "    ],\n",
    ")\n",
    "val_schema = Schema(\n",
    "    prediction_id_column_name='pred_id',\n",
    "    prompt_column_names=EmbeddingColumnNames(\n",
    "        data_column_name=\"text\",\n",
    "        vector_column_name=\"centered_text_vector\",\n",
    "    ),\n",
    "    response_column_names=EmbeddingColumnNames(\n",
    "        vector_column_name=\"centered_text_vector\",\n",
    "    ),\n",
    "    prediction_label_column_name='pred',\n",
    "    actual_label_column_name='pred'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = arize_client.log(\n",
    "    dataframe=query_df,\n",
    "    schema=prod_schema,\n",
    "    model_id=MODEL_ID,\n",
    "    model_version=\"1.0\",\n",
    "    model_type=ModelTypes.GENERATIVE_LLM,\n",
    "    environment=Environments.PRODUCTION\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(f\"✅ Successfully logged data to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'❌ Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.reset_index(drop=True, inplace=True)\n",
    "response = arize_client.log(\n",
    "    schema=val_schema,\n",
    "    dataframe=database_df,\n",
    "    model_id=MODEL_ID,\n",
    "    model_version=\"1.0\",\n",
    "    model_type=ModelTypes.GENERATIVE_LLM,\n",
    "    environment=Environments.VALIDATION,\n",
    "    batch_id=\"batch 1\"\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(f\"✅ Successfully logged data to Arize!\")\n",
    "else:\n",
    "    print(\n",
    "        f'❌ Logging failed with status code {response.status_code} and message \"{response.text}\"'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
