{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg0vEoEWFbDK"
   },
   "source": [
    "# Arize Tutorial: Wine Quality Multi-Classification Model  \n",
    "\n",
    "Let's get started on using Arize!âœ¨\n",
    "\n",
    "Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models. \n",
    "\n",
    "In this tutorial, we will be building a model to predict the quality categorical of the wine. The model is a Multi-Classification model with 3 different quality levels: 0, 1, or 2. We will load the models's training inferences and test inferences into Arize. ðŸš€. \n",
    "\n",
    "\n",
    "### Running This Notebook\n",
    "1. Click \"Open in playground\" to create a copy of this notebook for yourself.\n",
    "2. Save a copy in **Google Drive** for yourself.\n",
    "3. Step through each section below, pressing play on the code blocks to run the cells.\n",
    "4. In Step 2, use your own Space and API key from your Arize account. \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Arize-ai/client_python/blob/main/arize/examples/tutorials/Arize_Tutorial_PyTorch_Multi_class_Classification_Wine_Quality.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk-YaWFKCESF"
   },
   "source": [
    "## Step 1: Load Data and Build Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import concurrent.futures as cf\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1 Load Data and split data\n",
    "data = load_wine()\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# 1.5 In ModelTypes.SCORE_CATEGORICAL, we expect labels to be str\n",
    "y = y.astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42)\n",
    "\n",
    "# 2 Fit an classifier\n",
    "clf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# 3 Use the model to generate predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print('Step 1 âœ…: Load Data & Build Model Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOKdHjhFCMqe"
   },
   "source": [
    "## Step 2: Import and Setup Arize Client\n",
    "You can find your `API KEY` and `SPACE ID` by navigating to the space settings page in your workspace (only space admins can see the keys). Copy those over to the set-up section. We will also be setting up some metadata to use across all logging.\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-id-and-key.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize -q\n",
    "from arize.api import Client\n",
    "from arize.utils.types import ModelTypes\n",
    "\n",
    "SPACE_ID = 'SPACE_ID'\n",
    "API_KEY = 'API_KEY'\n",
    "arize = Client(space_id=SPACE_ID, api_key=API_KEY)\n",
    "\n",
    "model_id = 'wine_quality'\n",
    "model_version = '1.0'\n",
    "model_type = ModelTypes.SCORE_CATEGORICAL\n",
    "\n",
    "print('Step 2 âœ…: Import and Setup Arize Client Done! Now we can start using Arize!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKBhJmAJJfs2"
   },
   "source": [
    "## Step 3: Log Training Inferences to Arize\n",
    "First step: Log the training data for your model to Arize! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training\n",
    "train_prediction_labels = pd.Series(y_train_pred)\n",
    "train_actual_labels = pd.Series(y_train)\n",
    "train_feature_df = pd.DataFrame(X_train, columns=data['feature_names'])\n",
    "\n",
    "train_responses = arize.log_training_records(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=model_type, # this will change depending on your model type\n",
    "    prediction_labels=train_prediction_labels,\n",
    "    actual_labels=train_actual_labels,\n",
    "    features=train_feature_df,\n",
    "    )\n",
    "    \n",
    "## Helper to listen to response code to ensure successful delivery\n",
    "def arize_responses_helper(responses): \n",
    "  for response in cf.as_completed(responses):\n",
    "    res = response.result()\n",
    "    if res.status_code != 200:\n",
    "      print(f'future failed with response code {res.status_code}, {res.text}')\n",
    "\n",
    "arize_responses_helper(train_responses)\n",
    "\n",
    "print('Step 3 âœ…: If no errors showed up, you have sent Training Inferences!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmi9WYpDoS4"
   },
   "source": [
    "## Step 4: Log Validation Inferences to Arize\n",
    "Next Step: Log the validation data. You need to include `batch_id` to separate out different validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging validation\n",
    "val_prediction_labels = pd.Series(y_val_pred)\n",
    "val_actual_labels = pd.Series(y_val)\n",
    "val_features_df = pd.DataFrame(X_val, columns=data['feature_names'])\n",
    "\n",
    "val_responses = arize.log_validation_records(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=model_type,\n",
    "    batch_id='batch0',\n",
    "    prediction_labels=val_prediction_labels,\n",
    "    actual_labels=val_actual_labels,\n",
    "    features=val_features_df,\n",
    "    )\n",
    "\n",
    "arize_responses_helper(val_responses)\n",
    "print('Step 4 âœ…: If no errors showed up, you have sent Validation Inferences!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZM2he8Q1H2l"
   },
   "source": [
    "## Step 5: Log Production Inferences to Arize  \n",
    "Note: We will be sending our test data to emulate sending production data. \n",
    "\n",
    "1. **Time-Override:** You can directly specify the unix time which the prediction was made by using the optional argument `time_overwrite`. This will set the prediction timestamp. It can be a `pd.Series` or `List` of dtype `int` and the same length as the predictions sent. The timestamp at index 1 will be the prediction timestamp at index 1. You can set the prediction timestamp within the last year.\n",
    "\n",
    "In our example, we used `time_overwrite` to simulate predictions over 30 days so you can see it displayed on arize platform right away.\n",
    "\n",
    "2. For production, we use `log_bulk_predictions` \n",
    "and `log_bulk_actuals`. You can log the ground truth label after logging predictions at any time by matching the `predictions_ids` represented by `ids_df` here in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "test_prediction_labels = pd.Series(y_test_pred) # can also be pd.DataFrame()\n",
    "test_actual_labels = pd.Series(y_test)\n",
    "test_feature_df = pd.DataFrame(X_test, columns=data['feature_names'])\n",
    "\n",
    "# ids_df will be used to match actuals after predictions have already been generated\n",
    "ids_df = pd.DataFrame([str(uuid.uuid4()) for _ in range(len(y_test_pred))])\n",
    "\n",
    "# OPTIONAL: Simulate predictions evenly distributed over 30 days by manually specifying prediction time\n",
    "current_time = datetime.datetime.now().timestamp()\n",
    "earlier_time = (datetime.datetime.now() - datetime.timedelta(days=30)).timestamp()\n",
    "optional_time_overwrite = np.linspace(earlier_time, current_time, num=len(y_test_pred))\n",
    "optional_time_overwrite = pd.Series(optional_time_overwrite.astype(int))\n",
    "\n",
    "# First we log the predictions. We are using log_bulk_predictions since we are sending more than 1 prediction. \n",
    "prediction_responses = arize.log_bulk_predictions(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=model_type,\n",
    "    prediction_ids=ids_df,\n",
    "    prediction_labels=test_prediction_labels,\n",
    "    features=test_feature_df,\n",
    "    time_overwrite=optional_time_overwrite # this argument is optional\n",
    "    )\n",
    "\n",
    "arize_responses_helper(prediction_responses)\n",
    "\n",
    "print('Step 5 âœ…: If no errors showed up, you have successfully sent in {} Production Inferences!'.format(len(optional_time_overwrite)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyajKa1H1TU5"
   },
   "source": [
    "## Step 6: Logging Production Actuals to Arize\n",
    "\n",
    "We match the predictions above using `log_bulk_actuals` with the same `prediction_ids`. \n",
    "\n",
    "**Note:** You can use `log_bulk_predictions` for logging bulk predictions, and follow up with individual calls to the Arize API with `log_actual` instead of `log_bulk_actual` as labels become avaliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we log the actuals. We are using log_bulk_actuals since we are sending more than 1 actual. \n",
    "test_actual_labels = pd.Series(y_test)\n",
    "\n",
    "actual_responses = arize.log_bulk_actuals(\n",
    "    model_id=model_id,\n",
    "    prediction_ids=ids_df, # Pass in the same IDs to match the predictions & actuals. \n",
    "    actual_labels=test_actual_labels\n",
    "    )\n",
    "\n",
    "arize_responses_helper(actual_responses)\n",
    "print('Step 6 âœ…: If no errors showed up, you have successfully sent in Actuals!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJnu5q-21ZBK"
   },
   "source": [
    "# Step 7: Generating and Formatting SHAP Values\n",
    "**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model.\n",
    "\n",
    "For more in-depth usage of the `shap` library, visit [SHAP Core Explainers](https://shap-lrjball.readthedocs.io/en/docs_update/generated/shap.Explainer.html) and pick an explainer specific to your machine learning model. `shap.Explainer` is the default explainer that will matches model type, but you can specify your own type. For example, you can choose to use for example `shap.TreeExplainer`, but it won't work on models such as `sklearn.LinearModel.LogisticRegression`.\n",
    "\n",
    "We create this helper function `get_shap_values` to format the data and/or create visualizations for our shap values. We will store our results in a `pd.DataFrame` with matching columns for logging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "import shap\n",
    "\n",
    "def get_shap_values(model, X_data, ExplainerType=shap.Explainer, show_graph=False):\n",
    "    # NOTE: If there are errors, you  need to manually choose which explainer class\n",
    "    explainer = ExplainerType(model, X_data)\n",
    "    shap_values = explainer.shap_values(X_data, check_additivity=False)\n",
    "\n",
    "    # When not in production, it can be helpful to check graphs for feature explainability\n",
    "    if show_graph:\n",
    "        shap.summary_plot(shap_values, X_data, feature_names=data['feature_names'])\n",
    "        shap.summary_plot(np.sum(np.array(shap_values), 0), X_data, feature_names=data['feature_names'])\n",
    "\n",
    "    # Since we are using TreeExplainer, we need to manually log this\n",
    "    shap_values = np.sum(np.array(shap_values), 0)\n",
    "    \n",
    "    # NOTE: Arize API expects a DataFrame of the same shape and column name as the model features.\n",
    "    return pd.DataFrame(shap_values, columns=data['feature_names'])\n",
    "\n",
    "shap_values = get_shap_values(clf, X, ExplainerType=shap.TreeExplainer, show_graph=True)\n",
    "print('Step 7 âœ…: If no errors showed up, you should see SHAP Graphs above here!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS2F2SVt1eZc"
   },
   "source": [
    "# Step 8: Logging SHAP Values to Arize\n",
    "Similar to `log_bulk_actuals`, log the SHAP values using the `shap_df` our helper function generated in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = get_shap_values(clf, X_test) # custom function defined earlier\n",
    "\n",
    "shap_responses = arize.log_bulk_shap_values(\n",
    "    model_id=model_id,\n",
    "    prediction_ids=ids_df,\n",
    "    shap_values=shap_df\n",
    "    )\n",
    "\n",
    "arize_responses_helper(shap_responses)\n",
    "print('Step 8 âœ…: If no errors showed up, you have sent SHAP values to Arize!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Workspace\n",
    "\n",
    "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n",
    "\n",
    "https://www.arize.com"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
