{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0TvdSg5HC3Q"
   },
   "source": [
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "\n",
    "# Getting Started with the Arize Platform - Ranking\n",
    "\n",
    "**In this walk through, we are going to investigate the performance of a Hotel Booking Ranking model using nDCG.**\n",
    "\n",
    "Use case: You manage the Machine Learning models for a Hotel Booking company and spent a significant amount of time collecting online data and training your model. Now that your model is in production, you have little understanding of your model performance, but you notice a higher bounce rate than normal. Use Arize to identify your model issues and gain insights into how to improve your models.\n",
    "\n",
    "In this walk through we will look at a few scenarios common to a ranking use-case and more specifically looking at the NDCG at different @k values.\n",
    "\n",
    "You will learn to:\n",
    "\n",
    "1. Get training and production data into the Arize platform\n",
    "2. Setup threshold alerts\n",
    "3. Understand where the model is under performing\n",
    "4. Discover the root cause of issues\n",
    "\n",
    "The sample data contains 1 month of information in which 2 main characteristics exist. You will work on identifying these characteristics in the course of this exercise. At a glance:\n",
    "\n",
    "1. A new untrained domain is introduced\n",
    "2. Difference in NDCG performance at different K values\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "NDCG measures a model's ability to rank query results in the order of the highest relevance. Actual relevance scores are usually determined by user interaction. The k value determines the sum of gains up to position k in a list.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://1591756861-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MAlgpMyBRcl2qFZRQ67%2Fuploads%2FBtoFDYp86ob7FpqFEOi4%2F1_sb2sXH1RHQFgZgl4l9pCSw.png?alt=media&token=5997e120-65d9-4c88-9364-57d5c0bc16d2\"\n",
    "width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OHUs0p6HCsX"
   },
   "source": [
    "# Step 0. Setup and Getting the Data\n",
    "\n",
    "The first step is to load our preexisting dataset which includes training and production environments for our ranking model. Using a preexisting dataset illustrates how simple it is to get started with the Arize platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDVA4kK5HHqa"
   },
   "source": [
    "## Install Dependencies and Import Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize\n",
    "\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from arize.pandas.logger import Client\n",
    "from arize.utils.types import Environments, ModelTypes, Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C8bwEnXHHnx"
   },
   "source": [
    "## **üåê Download the Data**\n",
    "In this walk through, we‚Äôll be sending real historical data. Note, that while feature names and values are made explicit in this dataset, you can achieve the same level of ML Observability using obfuscated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/ndcg_ranking_production_dataset.parquet\",\n",
    ")\n",
    "train = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/ndcg_ranking_training_dataset.parquet\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dependencies installed and data successfully downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG0pz7_AdZWb"
   },
   "source": [
    "## Inspect the Data\n",
    "\n",
    "Take a quick look at the dataset. The data represents a model designed\n",
    "to predict the likelihood a user clicks on a recommended hotel in an ordered list. The model's predictions are based on features such as destination, location, country, etc. This dataset contains one month of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production[\"actual_relevancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_column_names = [\n",
    "  'prop_log_historical_price',\n",
    "  'price_usd',\n",
    "  'promotion_flag',\n",
    "  'search_destination_id',\n",
    "  'search_length_of_stay',\n",
    "  'search_booking_window',\n",
    "  'search_adults_count',\n",
    "  'search_children_count',\n",
    "  'search_room_count',\n",
    "  'search_saturday_night_bool',\n",
    "  'destination'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgJKldiSH6OL"
   },
   "source": [
    "# Step 1. Sending Data into Arize üí´\n",
    "\n",
    "Now that we have our dataset imported, we are ready to integrate into Arize. We do this by logging (sending) important data we want to analyze to the platform. There, the data will be easily visualized and troubleshooting workflows will help us find the source of our problem.\n",
    "\n",
    "For our model, we are going to log:\n",
    "*   feature data\n",
    "*   group id\n",
    "*   rank\n",
    "*   actuals\n",
    "## Import and Setup Arize Client\n",
    "\n",
    "The first step is to set up our Arize client. After that we will log the data.\n",
    "\n",
    "First, copy the Arize `API_KEY` and `SPACE_KEY` from your Space Settings page shown below! Copy those over to the set-up section. We will also be setting up some metadata to use across all logging.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w80Dqb3lqKIJ"
   },
   "source": [
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_KEY = \"SPACE_KEY\"\n",
    "API_KEY = \"API_KEY\"\n",
    "\n",
    "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
    "\n",
    "model_id = (\n",
    "    \"ranking-demo-model\"  # This is the model name that will show up in Arize\n",
    ")\n",
    "model_version = \"v1.0\"  # Version of model - can be any string\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"‚úÖ Arize setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVIZgl2oIHGt"
   },
   "source": [
    "## Log Training & Production Data to Arize\n",
    "\n",
    "Now that our Arize client is set up, let's go ahead and log all of our data to the platform. For more details on how **`arize.pandas.logger`** works, visit out documentations page below.\n",
    "\n",
    "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)\n",
    "\n",
    "*   **prediction_group_id_column_name**: This is the query id for each ranking groups or lists in ranking models (Required).\n",
    "*   **rank_column_name**: Rank of each element within its group or list (Required).\n",
    "*   **actual_label_column_name**: A list of strings that represent multiple engagement actions of each element (at least one of Relevance Scores or Actual Label are required).\n",
    "*   **actual_score_column_name**: Scores are generated base on the engagement actions of each element (at least one of Relevance Scores or Actual Label are required).\n",
    "* **prediction_score_column_name**: The prediction scores used to generate the rank (Optional).\n",
    "* **prediction_label_column_name**: Set to \"relevant\" (since only relevant results are displayed)  (Optional).\n",
    "\n",
    "Given that our model is predicting between categories, we will use [ModelTypes.RANKING](https://docs.arize.com/arize/product-guides-1/models/model-types) to perform this analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbFoahq4IMfd"
   },
   "source": [
    "## Log Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
    "training_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    prediction_group_id_column_name = \"search_id\",\n",
    "    rank_column_name = \"rank\",\n",
    "    actual_label_column_name = \"actual_relevancy\",\n",
    "    feature_column_names=feature_column_names,\n",
    ")\n",
    "\n",
    "# Logging Training DataFrame\n",
    "training_response = arize_client.log(\n",
    "    dataframe=train,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=ModelTypes.RANKING,\n",
    "    environment=Environments.TRAINING,\n",
    "    schema=training_schema,\n",
    ")\n",
    "\n",
    "# If successful, the server will return a status_code of 200\n",
    "if training_response.status_code != 200:\n",
    "    print(\n",
    "        f\"logging failed with response code {training_response.status_code}, {training_response.text}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚úÖ You have successfully logged training set to Arize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRQdEXZDIvMo"
   },
   "source": [
    "## Log Production Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing dates for ease of visualization / to mimic recent production dataset\n",
    "END_DATE = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "START_DATE = (datetime.date.today() - timedelta(31)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def set_prediction_id_and_time(df, start, end):\n",
    "    out_df = pd.DataFrame()\n",
    "    dts = pd.date_range(start, end).to_pydatetime().tolist()\n",
    "    for dt in dts:\n",
    "        day_df = df.loc[df[\"day\"] == dt.day].copy()\n",
    "        day_df[\"prediction_ts\"] = int(dt.strftime(\"%s\"))\n",
    "        out_df = pd.concat([out_df, day_df], ignore_index=True)\n",
    "    out_df[\"prediction_id\"] = [str(uuid.uuid4()) for _ in range(out_df.shape[0])]\n",
    "    return out_df.drop(columns=\"day\")\n",
    "\n",
    "\n",
    "production = set_prediction_id_and_time(production, START_DATE, END_DATE)\n",
    "\n",
    "production_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_group_id_column_name = \"search_id\",\n",
    "    rank_column_name = \"rank\",\n",
    "    actual_label_column_name = \"actual_relevancy\",\n",
    "    feature_column_names=feature_column_names,\n",
    ")\n",
    "\n",
    "production_response = arize_client.log(\n",
    "    dataframe=production,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_type=ModelTypes.RANKING,\n",
    "    environment=Environments.PRODUCTION,\n",
    "    schema=production_schema,\n",
    ")\n",
    "\n",
    "if production_response.status_code != 200:\n",
    "    print(\n",
    "        f\"logging failed with response code {production_response.status_code}, {production_response.text}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚úÖ You have successfully logged production set to Arize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GRDmDYmJWFt"
   },
   "source": [
    "# Step 2. Confirm Data in Arize ‚úÖ\n",
    "\n",
    "Note that the Arize performs takes about 10 minutes to index the data. While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to head over to the **Data Ingestion** tab for your model to watch Arize work its magic!üîÆ\n",
    "\n",
    "You will be able to see the predictions, actuals, and feature importances that have been sent in the last 30 minutes, last day or last week.\n",
    "\n",
    "An example view of the Data Ingestion tab from a model, when data is sent continuously over 30 minutes, is shown in the image below.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/data-ingestion-tab.png\" width=\"700\">\n",
    "\n",
    "Note that the Arize performs takes about 10 minutes to index the data. While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to head over to the **Data Ingestion** tab for your model to watch Arize works its magic!üîÆ\n",
    "\n",
    "**‚ö†Ô∏è DON'T SKIP:**\n",
    "In order to move on to the next step, make sure your actuals and training/production sets are loaded into the platform. To check:\n",
    "1. Navigate to models from the left bar, locate and click on model **ranking-demo-model**\n",
    "2. On the **Overview Tab**, make sure you can see Predictions and Actuals under the **Model Health** section. Once production actuals have been fully recorded on Arize, the row title will change from **0 Actuals** to **Actuals** with summary statistics such as cardinality listed in the tables.\n",
    "3. Verify the list of **Categorical** and **Numeric Features** below **Actuals**.\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/model_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkgBBB0IMHmv"
   },
   "source": [
    "# Step 3. Set up Model Baseline\n",
    "\n",
    "Now that our data has been logged into the [Arize platform](https://app.arize.com/) we can begin our investigation into our poorly performing Hotel search ranking model.\n",
    "\n",
    "First, set the baseline to the training set that we logged before.\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/model_baseline.gif)\n",
    "\n",
    "Under the **Config** tab we can also set **Performance Configs**. Select the following configurations:\n",
    "\n",
    "\n",
    "1.   Default Metric: `NDCG`\n",
    "2.   Default @K value: 10\n",
    "3.   Positive Class: `Relevant`\n",
    "\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/performance_config.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0o72uxwN1y6"
   },
   "source": [
    "# Step 4. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0txSoJdEAFh"
   },
   "source": [
    "Let's begin troubleshooting our model by navigating to the **Performance Tracing** page. Here, we notice our nDCG value is extremely low for the first 10 recommendations in our model, where our @k value = 10\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/perf_tracing.png)\n",
    "\n",
    "To further troubleshoot our performance degradation, we'll increase our @k value to see if there's a change in performance for lower ranked recommendations from 10 to 20. This helps us gauge where our model failure is within a list of ranks.\n",
    "\n",
    "When we change our k value to 20, we can see a significant improvement in our model performance. To further investigate model performance, we'll add a **comparison model** which will help us evaluate our performance breakdown chart.\n",
    "\n",
    "To add a comparison model, navigate to the top of the 'Performance Tracing' page and click on add comparison. Since we want to compare our production data with our training data, toggle 'production' to **'training'** to populate an additional dataset.\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/ranking_vid1.gif)\n",
    "\n",
    "Now that we've populated our training dataset as 'Dataset B', we can compare how features impact performance between two environments. Scroll down to the **'Performance Breakdown'** card to compare features between our two datasets.\n",
    "\n",
    "When comparing two histograms look for:\n",
    "- different colors (the more red = worse performing)\n",
    "- missing values\n",
    "\n",
    "As we scroll through our model's feature comparisons, notice how `DESTINATION` has a gap between the two histograms. From here, we can click into the card for a detailed view of exactly what's missing.\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/fixtures/Ranking-Use-Case/ranking_recording2.gif)\n",
    "\n",
    "Our performance breakdown comparison indicates our training data is missing `DESTINATION` data for `JACKSONHOLE`,`BREKENRIDGE`,`VAIL`,`ASPEN`, and `PARK CITY`. To confirm our data quality issues, click on the **'View Feature Details'** link on the top right, which will navigate us to a page where we visualize our distribution comparison, cardinality, and % empty over time.\n",
    "\n",
    "This ultimately indicates that our training set is not predicting for winter locations, and will likely need to be **retrained** to account for winter locations in the new training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jZ9lFAebcpX"
   },
   "source": [
    "# üìö Conclusion\n",
    "In this walk through we've shown how to log prediction data, pinpoint model performance degradation, and using a rank-aware evaluation metric to improve our model.\n",
    "\n",
    "We completed the following tasks:\n",
    "1. Uploaded data from a ranking model\n",
    "2. Set up a model baseline, evaluated different @k values, added a comparison dataset, and root cause our model's issue\n",
    "3. Compared production data against training data to identify problematic areas in the performance heatmap\n",
    "4. Identified correlations between our model's degrading performance with data quality issues in training data\n",
    "\n",
    "\n",
    "We identified the following areas of concern:\n",
    "1. nDCG performance @k = 10 performs worse than higher @k values\n",
    "2. Missing training data for `DESTINATION`\n",
    "3. Poor data quality for `DESTINATION` -- with `JACKSONHOLE`,`BREKENRIDGE`,`VAIL`,`ASPEN`, and `PARK CITY` missing compared to production data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es52tD8lbceL"
   },
   "source": [
    "# About Arize\n",
    "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
    "- Automated ML monitoring and model monitoring\n",
    "- Workflows to troubleshoot model performance\n",
    "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
    "- Model prediction cohort analysis\n",
    "- Pre-deployment model validation\n",
    "- Integrated model explainability\n",
    "\n",
    "### Website\n",
    "Visit Us At: https://arize.com/model-monitoring/\n",
    "\n",
    "### Additional Resources\n",
    "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
    "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
    "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
    "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
    "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
    "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
    "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
    "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
    "\n",
    "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
