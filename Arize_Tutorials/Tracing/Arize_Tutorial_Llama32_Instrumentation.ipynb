{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588eec5d-f95a-4a40-af8d-7923c23d39d6",
   "metadata": {
    "id": "588eec5d-f95a-4a40-af8d-7923c23d39d6"
   },
   "source": [
    "# <center>Tracing Llama 3.2 with the OpenAI API </center>\n",
    "This guide demonstrates how to use trace open-source models like Llama 3.2, utilizing the OpenAI API.\n",
    "\n",
    "To instrument an open-source Llama model, Ollama has built-in compatibility with the OpenAI [Chat Completions API](https://github.com/ollama/ollama/blob/main/docs/openai.md), making it possible to use more tooling and applications with open-source models locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aaa01-f8eb-439e-85a9-4e87a83e95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-otel colab-xterm ollama \"openai>=1.26\" openinference-instrumentation-openai opentelemetry-sdk opentelemetry-exporter-otlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1pywoSqFyYQ0",
   "metadata": {
    "id": "1pywoSqFyYQ0"
   },
   "source": [
    "### Installing Ollama\n",
    "\n",
    "Download and execute the installation script from the Ollama website. The script will handle the installation process automatically, including downloading and installing necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jd_O1lqWuXUZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xl4mKLnVyF5G",
   "metadata": {
    "id": "xl4mKLnVyF5G"
   },
   "source": [
    "### Launching Xterm\n",
    "\n",
    "\n",
    "Launch the xterm terminal within the Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HhC-PUWhwBQ7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iD6_T9YtynIY",
   "metadata": {
    "id": "iD6_T9YtynIY"
   },
   "source": [
    "### Launch Terminal & Start the Ollama Server\n",
    "Once Ollama is installed and the terminal is running, we can start the server using the following command. Be sure to run this in the `xterm` terminal above!\n",
    "\n",
    "```shell\n",
    "ollama serve &\n",
    "```\n",
    "\n",
    "The `&` at the end runs the command in the background, allowing you to continue using your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mizH98mqwCs5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AN74saWz0z4c",
   "metadata": {
    "id": "AN74saWz0z4c"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe757dc-09e9-4556-8713-3cb0b3ef02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from arize_otel import Endpoints, register_otel\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenInference - Instrumentation\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6Ry-HLOy5vt",
   "metadata": {
    "id": "T6Ry-HLOy5vt"
   },
   "source": [
    "### Download Llama 3.2\n",
    "\n",
    "Using the `ollama` library , we can request the `llama3.2:1b` model to run in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X5OCPJv9wgnA",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_MODEL_NAME = \"llama3.2:1b\"\n",
    "\n",
    "ARIZE_MODEL_ID = f\"arize_{LLAMA_MODEL_NAME}_openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TpmneBW4t4Hr",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(LLAMA_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P4ZTqUF9zF5r",
   "metadata": {
    "id": "P4ZTqUF9zF5r"
   },
   "source": [
    "### Register OTEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293e053-3547-4dbc-a456-0bda6f8432a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_otel(\n",
    "    endpoints=Endpoints.ARIZE,\n",
    "    space_id=\"U3BhY2U6NTI3MjpoVDJX\",  # in app space settings page\n",
    "    api_key=\"2b8d908de9c051dbd1f\",  # in app space settings page\n",
    "    model_id=ARIZE_MODEL_ID,  # name this to whatever you would like\n",
    "    model_version=\"1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15802f98-856f-4580-bda5-3f8e3e863544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrument OpenAI calls in your application\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AtMyzsITzMBJ",
   "metadata": {
    "id": "AtMyzsITzMBJ"
   },
   "source": [
    "### Create OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceadac6-df16-45d6-949d-fcfe54618d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",  # required, but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7QnOYmRDzSxU",
   "metadata": {
    "id": "7QnOYmRDzSxU"
   },
   "source": [
    "### Run Queries\n",
    "\n",
    "Run queries against `llama3.2:1b`, using the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdd69d-95e6-4ee4-a06a-c2fe0fef746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_query(oai_client: OpenAI, model_name: str, query: str):\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045453c0-760b-49ed-afb8-aa6d3dece7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_questions = [\n",
    "    \"What are Large Language Models?\",\n",
    "    \"How do large language models work?\",\n",
    "    \"How are LLMs trained, and what data is used?\",\n",
    "    \"In a large language model, what is a hallucination?\",\n",
    "    \"What are the main applications of large language models?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48589948-cd6c-46e1-a0cb-6a524feddaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in tqdm(lst_questions):\n",
    "    llm_response = ollama_query(\n",
    "        oai_client=oai_client, model_name=LLAMA_MODEL_NAME, query=question\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owU9I-tl4v0j",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
