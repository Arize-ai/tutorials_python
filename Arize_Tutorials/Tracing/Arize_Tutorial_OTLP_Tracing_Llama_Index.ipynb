{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f8e85c-70c2-4de3-99b8-acdbb58d6c4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center> <img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/> </center>\n",
    "\n",
    "# <center>Tracing via OTLP using Arize</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4db8b-dc93-4cef-ac58-205d1ec21b36",
   "metadata": {},
   "source": [
    "This guide demonstrates how to use Arize for monitoring and debugging your LLM using Traces and Spans. We're going to build a simple query engine using LlamaIndex and retrieval-augmented generation (RAG) to answer questions about the [Arize documentation](https://docs.arize.com/arize/). You can read more about LLM tracing [here](https://docs.arize.com/arize/llm-large-language-models/llm-traces). Arize makes your LLM applications observable by visualizing the underlying structure of each call to your query engine and surfacing problematic `spans` of execution based on latency, token count, or other evaluation metrics.\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. Use opentelemetry and [openinference](https://github.com/Arize-ai/openinference/tree/main) to instrument our application and sent traces via OTLP to Arize.\n",
    "2. Build a simple query engine using LlamaIndex that uses RAG to answer questions about the Arize documentation\n",
    "3. Inspect the traces and spans of your application to identify sources of latency and cost\n",
    "\n",
    "â„¹ï¸ This notebook requires:\n",
    "- An OpenAI API key\n",
    "- An Arize Space & API Key (explained below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f02b0-f638-4da8-a72d-371b07a5a28c",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies ðŸ“š\n",
    "Let's get the notebook setup with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398520d-47d5-450e-a0c6-3969ede28626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies needed to build the Llama Index RAG application\n",
    "!pip install gcsfs openai>=1 llama-index>=0.10.3 \n",
    "\n",
    "# Dependencies needed to export spans and send them to our collectors: Arize\n",
    "!pip install opentelemetry-exporter-otlp openinference-instrumentation-llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0c55e-69f0-4d81-b65e-13388866b467",
   "metadata": {},
   "source": [
    "## Step 2: OTLP Instrumentation\n",
    "Let's import the dependencies we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4eb78-f33b-4db9-8d60-ca22ef392f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ce7dc-55fa-44ce-b457-754c32b3d4fc",
   "metadata": {},
   "source": [
    "### Step 2.a: Define an exporter to Arize\n",
    "Creating an Arize exporter is very simple. We just need 2 things:\n",
    "* Space and API keys, that will be sends headers\n",
    "* Model ID and version, sent as resource attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bce764-0d42-4e9a-a86e-bee64a30a07c",
   "metadata": {},
   "source": [
    "Copy the Arize API_KEY and SPACE_KEY from your Space Settings page (shown below) to the variables in the cell below. We will also be setting up some metadata to use across all logging.\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3a52e-873c-4128-a183-a9db38f51305",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_KEY = \"SPACE_KEY\" # Change this line\n",
    "API_KEY = \"API_KEY\" # Change this line\n",
    "\n",
    "model_id = \"tutorial-otlp-tracing-llama-index-rag\"\n",
    "model_version = \"1.0\"\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"âŒ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"âœ… Import and Setup Arize Client Done! Now we can start using Arize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131d82c-3e83-4b0f-9845-f879af0dd641",
   "metadata": {},
   "source": [
    "Next, we create an OTLP exporter with the Arize endpoint detailed above. Note that we use GRPC to export traces to Arize, which acts as a collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e103c2-5b87-4ba3-9d8d-c250a748ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9e43-f046-4f50-8934-e874faff33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Space and API keys as headers\n",
    "os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS']=f\"space_key={SPACE_KEY},api_key={API_KEY}\"\n",
    "\n",
    "# Set the model id and version as resource attributes\n",
    "resource = Resource(\n",
    "    attributes={\n",
    "        \"model_id\":model_id,\n",
    "        \"model_version\":model_version,\n",
    "    }\n",
    ")\n",
    "\n",
    "endpoint = \"https://otlp.arize.com/v1\"\n",
    "span_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "span_processor = SimpleSpanProcessor(span_exporter=span_exporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd34ae-4b67-4a8a-b538-b5022635d0ca",
   "metadata": {},
   "source": [
    "### Step 2.c: Define a trace provider and initiate the instrumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cafd0-4490-4db0-a7b5-b7903ff9f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry import trace as trace_api\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257df62-2154-4de7-82c6-52cf6d3cf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(span_processor=span_processor)\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43f124-a8fd-41be-8a49-c82188f420c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running the instrumentation from a Colab environment, set skip_dep_check to True\n",
    "# For more information check https://github.com/Arize-ai/openinference/issues/100\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(skip_dep_check=IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aa150-82f5-4268-b7fd-95b059b03d59",
   "metadata": {},
   "source": [
    "## Step 3: Build Your Llama Index RAG Application ðŸ“\n",
    "Let's import the dependencies we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0731faa-f263-4441-9cee-50460b5842a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "from gcsfs import GCSFileSystem\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874e8d7-2a95-4547-8061-768e9acab805",
   "metadata": {},
   "source": [
    "Set your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29abbe-5bab-49b3-a643-c15a5d4f6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fbe94-f071-47f5-9ebb-3563560814ab",
   "metadata": {},
   "source": [
    "This example uses a `RetrieverQueryEngine` over a pre-built index of the Arize documentation, but you can use whatever LlamaIndex application you like. Download our pre-built index of the Arize docs from cloud storage and instantiate your storage context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04f2a9-cb92-4c7f-945f-0a629bdcbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    fs=file_system,\n",
    "    persist_dir=index_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ebe03-abbc-4545-95b7-4a7a5942cba2",
   "metadata": {},
   "source": [
    "We are now ready to instantiate our query engine that will perform retrieval-augmented generation (RAG). Query engine is a generic interface in LlamaIndex that allows you to ask question over your data. A query engine takes in a natural language query, and returns a rich response. It is built on top of Retrievers. You can compose multiple query engines to achieve more advanced capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf35c57-8399-4d31-8e57-735e0de2ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f99cc-f3e7-4b74-a613-6c0b1df70ef1",
   "metadata": {},
   "source": [
    "Let's test asking a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13d61d-3cab-4e07-a14b-357038646ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is Arize and how can it help me as an AI Engineer?\")     \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a4000-8267-44a1-a849-768167aa6624",
   "metadata": {},
   "source": [
    "Great! Our application works. Let's move on to the Observability Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4cadb-55b5-49b1-8bf3-8e4ef7a1a4f6",
   "metadata": {},
   "source": [
    "## Step 4: Use our instrumented query engine\n",
    "\n",
    "We will download a dataset of queries for our RAG application to answer and see the traces appear in Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096825c-ba77-4c44-9460-7b82a3de7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "queries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\n",
    "queries = []\n",
    "with urlopen(queries_url) as response:\n",
    "    for line in response:\n",
    "        line = line.decode(\"utf-8\").strip()\n",
    "        data = json.loads(line)\n",
    "        queries.append(data[\"query\"])\n",
    "\n",
    "queries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59677acb-788e-402d-ac5d-1f96b911d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 # Sample size\n",
    "qa_pairs = []\n",
    "for query in tqdm(queries[:N]):\n",
    "    resp = query_engine.query(query)\n",
    "    qa_pairs.append((query,resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e79697-36d1-4929-9413-05de9903d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q,a in qa_pairs:\n",
    "    q_msg = f\">> QUESTION: {q}\"\n",
    "    print(f\"{'-'*len(q_msg)}\")\n",
    "    print(q_msg)\n",
    "    print(f\">> ANSWER: {a}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
