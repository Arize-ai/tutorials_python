{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRXCErbBUeM8"
   },
   "source": [
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/Arize-Phoenix-header.jpg\" width=\"2000\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuuw7WlSUeM-"
   },
   "source": [
    "<center>\n",
    "    <h2>LLM Application Tracing & Evaluation Workflows</h2>\n",
    "    <h3>Exporting from Phoenix to Arize<br></h3>\n",
    "</center>\n",
    "\n",
    "\n",
    "This guide demonstrates how to use Arize for monitoring and debugging your LLM using Traces and Spans. We're going to use data from a Langchain agent.\n",
    "\n",
    "In this tutorial we will:\n",
    "1. Build a simple Langchain agent\n",
    "1. Set up [Phoenix](https://docs.arize.com/phoenix) as a [trace collector](https://docs.arize.com/phoenix/tracing/llm-traces) for the Langchain application\n",
    "2. Use Phoenix's [evals library](https://docs.arize.com/phoenix/evaluation/llm-evals) to compute LLM generated evaluations of our agent's responses\n",
    "3. Use arize SDK to export the traces and evaluations to Arize\n",
    "\n",
    "You can read more about LLM tracing in Arize [here](https://docs.arize.com/arize/llm-large-language-models/llm-traces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWxXseeZUeM-"
   },
   "source": [
    "## Step 1: Install Dependencies üìö\n",
    "Let's get the notebook setup with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies needed to build the Llama Index RAG application\n",
    "!pip install -qq gcsfs llama-index-llms-openai llama-index-embeddings-openai\n",
    "\n",
    "# Dependencies needed to export spans and send them to our collector: Phoenix\n",
    "!pip install -qq \"langchain>=0.0.334\"\n",
    "\n",
    "# Install Phoenix to generate evaluations\n",
    "!pip install -qq \"arize-phoenix[evals]\"\n",
    "\n",
    "# Install Arize SDK with `Tracing` extra dependencies to export Phoenix data to Arize\n",
    "!pip install -qq 'arize[Tracing]>=7.14.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZqmdXsjUeM_"
   },
   "source": [
    "## Step 2: Set up Phoenix as a Trace Collector in our LLM app\n",
    "\n",
    "To get started, launch the phoenix app. Make sure to open the app in your browser using the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyuibQsYUeM_"
   },
   "source": [
    "Once you have started a Phoenix server, you can start your Langchain application and configure it to send traces to Phoenix. To do this, you will have to instantiate Phoenix's LangChainInstrumentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cElRsZ2wUeM_"
   },
   "source": [
    "That's it! The Langchain application we build next will send traces to Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTEPI_7EUeNA"
   },
   "source": [
    "## Step 3: Build Your Langchain Application üìÅ\n",
    "\n",
    "We start by setting your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZKqc6CMUeNA"
   },
   "source": [
    "We will build a sample math agent as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "# Let's give the LLM access to math tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "agent_executor = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh6dDfzPUeNA"
   },
   "source": [
    "Let's chat with our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What is 47 raised to the 5th power?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50d6Z4rgUeNA"
   },
   "source": [
    "Great! Our application works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIa9Q9FeUeNA"
   },
   "source": [
    "## Step 4: Use the instrumented Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is (121 * 3) + 42?\",\n",
    "    \"what is 3 * 3?\",\n",
    "    \"what is 4 * 4?\",\n",
    "    \"what is 75 * (3 + 4)?\",\n",
    "    \"what is 23 times 87\",\n",
    "    \"what is 12 times 89\",\n",
    "    \"what is 3 to the power of 7?\",\n",
    "    \"what is 3492 divided by 9?\",\n",
    "    \"what is ((132*85)+(346/2))^3?\",\n",
    "    \"what is square root of 9801?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"> {query}\")\n",
    "    response = agent_executor.invoke({\"input\": query})\n",
    "    print(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvEAGDk6UeNB"
   },
   "source": [
    "## Step 5: Run Evaluations on the data in Phoenix\n",
    "\n",
    "We will use the phoenix client to extract data in the correct format for specific evaluations and the custom evaluators, also from phoenix, to run evaluations on our Langchain Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = px.Client().get_spans_dataframe(\"span_kind == 'AGENT'\")\n",
    "trace_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5rcwYHiUeNB"
   },
   "source": [
    "Next, we enable concurrent evaluations for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4610arFaUeNB"
   },
   "source": [
    "Then, we define our evaluators and run the evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "\n",
    "MY_CUSTOM_TEMPLATE = '''\n",
    "    You are evaluating the correctness of an LLM agent's responses to math questions.\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {attributes.input.value}\n",
    "    ************\n",
    "    [Response]: {attributes.output.value}\n",
    "    [END DATA]\n",
    "\n",
    "\n",
    "    Please focus on whether the answer to the math question is correct or not.\n",
    "    Your answer must be single word, either \"correct\" or \"incorrect\"\n",
    "    '''\n",
    "\n",
    "math_eval = llm_classify(\n",
    "    dataframe=trace_df,\n",
    "    template= MY_CUSTOM_TEMPLATE,\n",
    "    model=eval_model,\n",
    "    provide_explanation=True,\n",
    "    rails=[\"correct\",\"incorrect\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPKSeQkkUeNB"
   },
   "source": [
    "Finally, we log the evaluations into Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Math Eval\", dataframe=math_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd5euh8HUeNB"
   },
   "source": [
    "## Step 6: Export data to Arize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-UUyRXXUeNB"
   },
   "source": [
    "### Step 6.a: Get data into dataframes\n",
    "\n",
    "We extract the spans and evals dataframes from the phoenix client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = px.Client().get_trace_dataset()\n",
    "spans_df = tds.get_spans_dataframe(include_evaluations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = tds.get_evals_dataframe()\n",
    "evals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1tk2QbLUeNB"
   },
   "source": [
    "### Step 6.b: Initialize arize client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.pandas.logger import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUlRgzERUeNB"
   },
   "source": [
    "Sign up/ log in to your Arize account [here](https://app.arize.com/auth/login). Find your [space and API keys](https://docs.arize.com/arize/api-reference/arize.pandas/client). Copy/paste into the cell below.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_KEY = \"SPACE_KEY\" #Change this line\n",
    "API_KEY = \"API_KEY\" #Change this line\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"‚úÖ Import and Setup Arize Client Done! Now we can start using Arize!\")\n",
    "\n",
    "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
    "model_id = \"tuorial-tracing-with-evals-langchain-agent\"\n",
    "model_version = \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shu9jouVUeNF"
   },
   "source": [
    "Lastly, we use `log_spans` from the arize client to log our spans data and, if we have evaluations, we can pass the optional `evals_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = arize_client.log_spans(\n",
    "    dataframe=spans_df,\n",
    "    evals_dataframe=evals_df,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    ")\n",
    "\n",
    "# If successful, the server will return a status_code of 200\n",
    "if response.status_code != 200:\n",
    "    print(f\"‚ùå logging failed with response code {response.status_code}, {response.text}\")\n",
    "else:\n",
    "    print(f\"‚úÖ You have successfully logged traces set to Arize\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
