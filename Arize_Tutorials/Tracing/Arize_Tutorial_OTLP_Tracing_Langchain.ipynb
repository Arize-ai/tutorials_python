{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f8e85c-70c2-4de3-99b8-acdbb58d6c4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center> <img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/> </center>\n",
    "\n",
    "# <center>Tracing via OTLP using Arize</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4db8b-dc93-4cef-ac58-205d1ec21b36",
   "metadata": {},
   "source": [
    "This guide demonstrates how to use Arize for monitoring and debugging your LLM using Traces and Spans. We're going to build a simple question-and-answer application using LangChain and retrieval-augmented generation (RAG) to answer questions about the [Arize documentation](https://docs.arize.com/arize/). Arize makes your LLM applications observable by visualizing the underlying structure of each call to your query engine and surfacing problematic `spans` of execution based on latency, token count, or other evaluation metrics. You can read more about LLM tracing [here](https://docs.arize.com/arize/llm-large-language-models/llm-traces).\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. Use opentelemetry and [openinference](https://github.com/Arize-ai/openinference/tree/main) to instrument our application in order to send traces via OTLP to Arize.\n",
    "2. Build a simple question-and-answer application using LangChain and RAG to answer questions about the Arize documentation\n",
    "3. Inspect the traces and spans of your application to identify sources of latency and cost\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires:\n",
    "- An OpenAI API key\n",
    "- An Arize Space & API Key (explained below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f02b0-f638-4da8-a72d-371b07a5a28c",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies üìö\n",
    "Let's get the notebook setup with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398520d-47d5-450e-a0c6-3969ede28626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies needed to build the Langchain application\n",
    "!pip install -q \"langchain>=0.1.0\" langchain-openai\n",
    "\n",
    "# Dependencies needed to export spans and send them to our collectors: Arize\n",
    "!pip install -q opentelemetry-exporter-otlp 'openinference-instrumentation-langchain>=0.1.15'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0c55e-69f0-4d81-b65e-13388866b467",
   "metadata": {},
   "source": [
    "## Step 2: OTLP Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ce7dc-55fa-44ce-b457-754c32b3d4fc",
   "metadata": {},
   "source": [
    "### Step 2.a: Define an exporter to Arize\n",
    "Creating an Arize exporter is very simple. We just need 2 things:\n",
    "* Space and API keys, that will be sent as headers\n",
    "* Model ID and version, sent as resource attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bce764-0d42-4e9a-a86e-bee64a30a07c",
   "metadata": {},
   "source": [
    "Copy the Arize API_KEY and SPACE_KEY from your Space Settings page (shown below) to the variables in the cell below. We will also be setting up some metadata to use across all logging.\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3a52e-873c-4128-a183-a9db38f51305",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_KEY = \"SPACE_KEY\" # Change this line\n",
    "API_KEY = \"API_KEY\" # Change this line\n",
    "\n",
    "model_id = \"tutorial-otlp-tracing-langchain-rag\"\n",
    "model_version = \"1.0\"\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"‚úÖ Import and Setup Arize Client Done! Now we can start using Arize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131d82c-3e83-4b0f-9845-f879af0dd641",
   "metadata": {},
   "source": [
    "Next, we create an OTLP exporter with the Arize endpoint detailed above. Note that we use GRPC to export traces to Arize, which acts as a collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e5374-ba1d-4e9b-8ff2-1042bbd300b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9e43-f046-4f50-8934-e874faff33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Space and API keys as headers\n",
    "os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS']=f\"space_key={SPACE_KEY},api_key={API_KEY}\"\n",
    "\n",
    "# Set the model id and version as resource attributes\n",
    "resource = Resource(\n",
    "    attributes={\n",
    "        \"model_id\":model_id,\n",
    "        \"model_version\":model_version,\n",
    "    }\n",
    ")\n",
    "\n",
    "endpoint = \"https://otlp.arize.com/v1\"\n",
    "span_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "span_processor = SimpleSpanProcessor(span_exporter=span_exporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd34ae-4b67-4a8a-b538-b5022635d0ca",
   "metadata": {},
   "source": [
    "### Step 2.b: Define a trace provider and initiate the instrumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cafd0-4490-4db0-a7b5-b7903ff9f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry import trace as trace_api\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257df62-2154-4de7-82c6-52cf6d3cf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(span_processor=span_processor)\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2957-6ee3-4891-a097-0f2b8e3f6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running the instrumentation from a Colab environment, set skip_dep_check to True\n",
    "# For more information check https://github.com/Arize-ai/openinference/issues/100\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "LangChainInstrumentor().instrument(skip_dep_check=IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aa150-82f5-4268-b7fd-95b059b03d59",
   "metadata": {},
   "source": [
    "## Step 3: Build Your LangChain RAG Application üìÅ\n",
    "Let's import the dependencies we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0731faa-f263-4441-9cee-50460b5842a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874e8d7-2a95-4547-8061-768e9acab805",
   "metadata": {},
   "source": [
    "Set your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29abbe-5bab-49b3-a643-c15a5d4f6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fbe94-f071-47f5-9ebb-3563560814ab",
   "metadata": {},
   "source": [
    "This example uses a `RetrievalQA` chain over a pre-built index of the Arize documentation, but you can use whatever LangChain application you like.\n",
    "\n",
    "Download the pre-built index from cloud storage and instantiate your storage context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace5d7c-89d5-4a62-a615-d5bf82be057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/\"\n",
    "    \"unstructured/llm/context-retrieval/langchain-pinecone/database.parquet\"\n",
    ")\n",
    "knn_retriever = KNNRetriever(\n",
    "    index=np.stack(df[\"text_vector\"]),\n",
    "    texts=df[\"text\"].tolist(),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=chat_model_name)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=knn_retriever,\n",
    "    metadata={\"application_type\": \"question_answering\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f99cc-f3e7-4b74-a613-6c0b1df70ef1",
   "metadata": {},
   "source": [
    "Let's test our app by asking a question about the Arize documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13d61d-3cab-4e07-a14b-357038646ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"What is Arize and how can it help me as an AI Engineer?\")     \n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a4000-8267-44a1-a849-768167aa6624",
   "metadata": {},
   "source": [
    "Great! Our application works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4cadb-55b5-49b1-8bf3-8e4ef7a1a4f6",
   "metadata": {},
   "source": [
    "## Step 4: Use our instrumented chain\n",
    "\n",
    "We will download a dataset of queries for our RAG application to answer and see the traces appear in Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096825c-ba77-4c44-9460-7b82a3de7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "queries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\n",
    "queries = []\n",
    "with urlopen(queries_url) as response:\n",
    "    for line in response:\n",
    "        line = line.decode(\"utf-8\").strip()\n",
    "        data = json.loads(line)\n",
    "        queries.append(data[\"query\"])\n",
    "\n",
    "queries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59677acb-788e-402d-ac5d-1f96b911d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from openinference.instrumentation import using_attributes\n",
    "\n",
    "N1 = 5 # Number of traces for your first session\n",
    "SESSION_ID_1 = \"session-id-1\" # Identifer for your first session\n",
    "USER_ID_1 = \"john_smith\" # Identifer for your first session\n",
    "METADATA = {\n",
    "    \"key_bool\": True,\n",
    "    \"key_str\": \"value1\",\n",
    "    \"key_int\": 1\n",
    "}\n",
    "\n",
    "qa_pairs = []\n",
    "for query in tqdm(queries[:N1]):\n",
    "    with using_attributes(\n",
    "        session_id=SESSION_ID_1,\n",
    "        user_id=USER_ID_1,\n",
    "        metadata=METADATA,\n",
    "    ):\n",
    "        resp = chain.invoke(query)\n",
    "        qa_pairs.append((query,resp['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796bf09-729f-4a72-88f9-800b19da5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N2 = 3 # Number of traces for your second session\n",
    "SESSION_ID_2 = \"session-id-2\" # Identifer for your second session\n",
    "USER_ID_2 = \"jane_doe\" # Identifer for your second session\n",
    "\n",
    "for query in tqdm(queries[N1:N1+N2]):\n",
    "    with using_attributes(\n",
    "        session_id=SESSION_ID_2,\n",
    "        user_id=USER_ID_2,\n",
    "        metadata=METADATA\n",
    "    ):\n",
    "        resp = chain.invoke(query)\n",
    "        qa_pairs.append((query,resp['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e79697-36d1-4929-9413-05de9903d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q,a in qa_pairs:\n",
    "    q_msg = f\">> QUESTION: {q}\"\n",
    "    print(f\"{'-'*len(q_msg)}\")\n",
    "    print(q_msg)\n",
    "    print(f\">> ANSWER: {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c241d-43fe-4b3f-82c5-2ef7c32c37a0",
   "metadata": {},
   "source": [
    "## Step 5: Log into Arize and explore your application traces üöÄ\n",
    "\n",
    "Log into your Arize account, and look for the model with the same `model_id`. You are likely to see the following page if you are sending a brand new model. Arize is processing your data and your model will be accessible for you to explore your traces in no time. \n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/model-loading-tutorial-otlp-langchain.png\" width=\"700\"></center>\n",
    "\n",
    "After the timer is completed, you are ready to navigate and explore your traces\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/llm-tracing-overview-langchain.png\" width=\"700\"></center>\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/llm-tracing-detail-langchain.png\" width=\"700\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
