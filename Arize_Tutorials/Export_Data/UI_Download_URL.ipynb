{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZkuNcfyq9C2"
   },
   "source": [
    "# Debugging and Analyzing Data from Arize Platform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0FsAoeIBv5A"
   },
   "source": [
    "Use this template to explore, analyze, and debug using data from the Arize platform. It takes in the data export URL, which you enter below, and produces a clean pandas dataframe that can be used for analysis.\n",
    "\n",
    "\n",
    " ***Note: Make a copy of this notebook to allow edits***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZURhR9oAvZRP"
   },
   "source": [
    "## Setting up the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX5evpkLfx5B"
   },
   "source": [
    "Import libraries and define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def get_value(d):\n",
    "    for key, v in d.items():\n",
    "        if key == \"int\":\n",
    "            return int(v)\n",
    "        if key == \"double\":\n",
    "            return float(v)\n",
    "        return str(v) if key == \"string\" else v\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_up_dict_values(dict_to_clean):\n",
    "    for key in dict_to_clean:\n",
    "        if type(dict_to_clean[key]) == dict:\n",
    "            dict_to_clean[key] = get_value(dict_to_clean[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFcCCWHUr5Lc"
   },
   "source": [
    "**Edit parameters** with your export url and desired file preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the URL to your file (provided by Arize) here\n",
    "arize_ui_url = \"YOUR_DATA_EXPORT_URL\"\n",
    "file_name = \"downloaded_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R76je4iYsLiZ"
   },
   "source": [
    "Retrieve data from either the url or locally (if stored). Follow prompt instructions for authorization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(arize_ui_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTOPB8gjtl4i"
   },
   "source": [
    "Set up dataframe with the exported data.\n",
    " ***Attention: If your data is too big, Colab may not be able to accommodate all data in memory. You may need to use Colab Pro or attach this notebook to a high memory instance.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the formatted dataframe in this dictionary\n",
    "data_frame_dict = {}\n",
    "\n",
    "# open up the json file\n",
    "with open(file_name) as fp:\n",
    "\n",
    "    # read the data point into a dictionary\n",
    "    line = fp.readline()\n",
    "    index = 0\n",
    "\n",
    "    while line:\n",
    "\n",
    "        ## Reminder, if data is too large you may want to attach this notebook to a high memory instance. Alternatively, you can sample by uncommenting the code below:\n",
    "\n",
    "        # SAMPLE_SIZE = 10_000\n",
    "        # if index > SAMPLE_SIZE:\n",
    "        #     break\n",
    "\n",
    "        formatted_data_point = {}\n",
    "        data_point = json.loads(line)\n",
    "\n",
    "        # prediction\n",
    "        prediction_dict = data_point[\"prediction\"]\n",
    "\n",
    "        formatted_data_point[\"timestamp\"] = prediction_dict[\"timestamp\"]\n",
    "        formatted_data_point[\"modelVersion\"] = prediction_dict[\"modelVersion\"]\n",
    "        formatted_data_point[\"predictionId\"] = data_point[\"predictionId\"]\n",
    "\n",
    "        # features\n",
    "        if \"features\" in prediction_dict:\n",
    "            features = prediction_dict[\"features\"]\n",
    "            clean_up_dict_values(features)\n",
    "            for k in features:\n",
    "                formatted_data_point[k] = features[k]\n",
    "            del prediction_dict[\"features\"]\n",
    "        if \"tags\" in prediction_dict:\n",
    "            tags = prediction_dict[\"tags\"]\n",
    "            clean_up_dict_values(tags)\n",
    "            for tag in tags:\n",
    "                formatted_data_point[tag] = tags[tag]\n",
    "            del prediction_dict[\"tags\"]\n",
    "        # different model types are structured differently\n",
    "        prediction_label = \"label\" if \"label\" in prediction_dict else \"predictionLabel\"\n",
    "        if \"scoreCategorical\" in prediction_dict[prediction_label]:\n",
    "            cat = \"category\" if \"category\" in prediction_dict[prediction_label][\"scoreCategorical\"] else \"scoreCategory\"\n",
    "            prediction = prediction_dict[prediction_label][\"scoreCategorical\"][cat][\"category\"]\n",
    "            if \"score\" in prediction_dict[prediction_label][\"scoreCategorical\"][cat]:\n",
    "                score = float(\n",
    "                    prediction_dict[prediction_label][\"scoreCategorical\"][cat][\n",
    "                        \"score\"\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                score = 0\n",
    "            formatted_data_point[\"prediction_score\"] = score\n",
    "            formatted_data_point[\"prediction\"] = prediction\n",
    "        elif \"ranking\" in prediction_dict[prediction_label]:\n",
    "            ranking_prediction = prediction_dict[prediction_label][\"ranking\"]\n",
    "            formatted_data_point[\"prediction_group_id\"] = ranking_prediction[\"predictionGroupId\"]\n",
    "            if \"rank\" in ranking_prediction:\n",
    "              formatted_data_point[\"rank\"] = ranking_prediction[\"rank\"]\n",
    "            if \"prediction_score\" in ranking_prediction:\n",
    "              formatted_data_point[\"prediction_score\"] = ranking_prediction[\"prediction_score\"]\n",
    "            if \"label\" in ranking_prediction:\n",
    "              formatted_data_point[\"prediction\"] = ranking_prediction[\"label\"]\n",
    "        else:\n",
    "            clean_up_dict_values(prediction_dict)\n",
    "            prediction = prediction_dict[prediction_label]\n",
    "            formatted_data_point[\"prediction\"] = prediction\n",
    "\n",
    "        # actual\n",
    "        if \"actual\" in data_point:\n",
    "            actual_dict = data_point[\"actual\"]\n",
    "            if \"tags\" in actual_dict:\n",
    "                tags = actual_dict[\"tags\"]\n",
    "                clean_up_dict_values(tags)\n",
    "                for tag in tags:\n",
    "                    formatted_data_point[tag] = tags[tag]\n",
    "                del actual_dict[\"tags\"]\n",
    "            # different model types are structured differently\n",
    "            actual_label = \"label\" if \"label\" in actual_dict else \"actualLabel\"\n",
    "            if \"scoreCategorical\" in actual_dict[actual_label]:\n",
    "                cat = \"category\" if \"category\" in actual_dict[actual_label][\"scoreCategorical\"] else \"scoreCategory\"\n",
    "                actual = actual_dict[actual_label][\"scoreCategorical\"][cat][\"category\"]\n",
    "                if \"score\" in actual_dict[actual_label][\"scoreCategorical\"][cat]:\n",
    "                    score = float(\n",
    "                        actual_dict[actual_label][\"scoreCategorical\"][cat][\n",
    "                            \"score\"\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    score = 0\n",
    "                formatted_data_point[\"actual_score\"] = score\n",
    "                formatted_data_point[\"actual\"] = actual\n",
    "            elif \"ranking\" in actual_dict[actual_label]:\n",
    "                ranking_actual = actual_dict[actual_label][\"ranking\"]\n",
    "                if \"category\" in ranking_actual:\n",
    "                    formatted_data_point[\"actual\"] = ranking_actual[\"category\"][\"values\"]\n",
    "                if \"relevanceScore\" in ranking_actual:\n",
    "                    formatted_data_point[\"relevance_score\"] = float(ranking_actual[\"relevanceScore\"])\n",
    "                else:\n",
    "                    formatted_data_point[\"relevance_score\"] = 0.0\n",
    "            else:\n",
    "                clean_up_dict_values(actual_dict)\n",
    "                actual = actual_dict[actual_label]\n",
    "                formatted_data_point[\"actual\"] = actual\n",
    "\n",
    "        # add to new dataframe dict\n",
    "        data_frame_dict[index] = formatted_data_point\n",
    "\n",
    "        line = fp.readline()\n",
    "        index += 1\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(data_frame_dict)\n",
    "prediction_df = prediction_df.transpose()\n",
    "# Clean up - type timestamp to correct column type\n",
    "prediction_df[\"timestamp\"] = pd.to_datetime(prediction_df[\"timestamp\"])\n",
    "prediction_df[\"date_string\"] = prediction_df.timestamp.dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD4gWEBehL8z"
   },
   "source": [
    "Now the data is ready to be explored. Take a look at how it's formatted in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF_Hz0MRu6Bz"
   },
   "source": [
    "## Examples of breaking down the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdBluY0Jvf_9"
   },
   "source": [
    "### Count of prediction and actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ability to look at mean of prediction and actuals\n",
    "# If you are slicing on features in the platform this gives some examples how to slice on the same feature\n",
    "\n",
    "# Note this will not work in classification models where the predictions are True/False\n",
    "\"\"\"\n",
    "print(prediction_df['actual'].mean())\n",
    "print(prediction_df[(prediction_df['modelVersion'] == '1.0') ]['prediction'].mean())\n",
    "print(prediction_df[(prediction_df['modelVersion'] == '1.0') & (prediction_df.date_string > \"2021-03-20\")]['prediction'].mean())\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nscbto78wuDW"
   },
   "source": [
    "### MSE and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this will not work in classification models\n",
    "\"\"\"\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "print(mean_absolute_error(prediction_df['actual'], prediction_df['prediction']))\n",
    "slice_grade_3 = prediction_df[(prediction_df.date_string > \"2021-03-20\") ]\n",
    "print(mean_absolute_error(slice_grade_3['actual'], slice_grade_3['prediction']))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp8SCtn0yq5W"
   },
   "source": [
    "### Grouping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all the prediction data by the day they were made\n",
    "\"\"\"\n",
    "prediction_df.groupby(['date_string']).count()['prediction'].head()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxagsRorD3cB"
   },
   "source": [
    "## Workspace\n",
    "\n",
    "Expand this notebook as much as you need for your data digging needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "W7wFOeiPm8_q"
   },
   "source": [
    "## Overview\n",
    "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
    "- Automated ML monitoring and model monitoring\n",
    "- Workflows to troubleshoot model performance\n",
    "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
    "- Model prediction cohort analysis\n",
    "- Pre-deployment model validation\n",
    "- Integrated model explainability\n",
    "\n",
    "## Website\n",
    "Visit Us At: https://arize.com/model-monitoring/\n",
    "\n",
    "## Additional Resources\n",
    "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
    "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
    "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
    "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
    "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
    "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
    "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
    "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
    "\n",
    "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
