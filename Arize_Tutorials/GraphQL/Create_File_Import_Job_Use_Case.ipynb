{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a File Import Job Using the Arize GraphQL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the GraphQL Client using your developer API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gql[all]\n",
    "from gql import Client, gql\n",
    "from gql.transport.requests import RequestsHTTPTransport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your API key\n",
    "First, make sure you have developer permissions. If you are able to visit the [API explorer](https://app.arize.com/graphql), then you have developer permissions. If not, please ask your Account Admin to provide you with access. \n",
    "\n",
    "The API key can be retrieved from the [API explorer](https://app.arize.com/graphql) page. Click the button on the top right called \"Get Your API Key.\" A modal will pop up with your key, copy that into the `API_KEY` constant below. \n",
    "\n",
    "NOTE: this key is different than the SDK key used to send data to Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API_KEY\"\n",
    "\n",
    "# Select your transport with a defined URL endpoint\n",
    "transport = RequestsHTTPTransport(\n",
    "    url=\"https://app.arize.com/graphql/\", headers={\"x-api-key\": API_KEY}\n",
    ")\n",
    "\n",
    "# Create a GraphQL client using the defined transport\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Declare the mutation inputs\n",
    "The below inputs represent the creation of an import job for one particular sample use case. For an exhaustive list of mutation inputs and their descriptions, please consult the docs in the [API explorer](https://app.arize.com/graphql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spaces have globally unique IDs. You can get your space ID by visiting app.arize.com.\n",
    "# The url will be in this format: https://app.arize.com/organizations/:orgId/spaces/:spaceId\n",
    "# NOTE: this is not the same as the space key used to send data using the SDK\n",
    "SPACE_ID = \"SPACE_ID\"\n",
    "MODEL_NAME = \"YOUR_MODEL_NAME\"\n",
    "MODEL_TYPE = \"YOUR_MODEL_TYPE\"\n",
    "MODEL_ENVIRONMENT_NAME = \"YOUR_MODEL_ENVIRONMENT\"\n",
    "BLOB_STORE = \"YOUR_BLOB_STORE\"\n",
    "BUCKET_NAME = \"YOUR_STORAGE_BUCKET_NAME\"\n",
    "PREFIX = \"YOUR_FILE_PATH\"\n",
    "\n",
    "# The schema maps the file contents to the model inferences. For more information about this mapping, please consult https://docs.arize.com/arize/data-ingestion/object-store-integration/file-schema\n",
    "# An example schema is provided below - this schema represents a file containing prediction id, prediction label, timestamp, and actual label columns. Since \"features\" is not included as a field, all\n",
    "# other non-reserved columns in the schema declaration will be inferred to be features.\n",
    "SCHEMA = {\n",
    "    \"predictionId\": \"YOUR_PREDICTION_ID_COLUMN_NAME\",\n",
    "    \"predictionLabel\": \"YOUR_PREDICTION_LABEL_COLUMN_NAME\",\n",
    "    \"timestamp\": \"YOUR_TIMESTAMP_COLUMN_NAME\",\n",
    "    \"actualLabel\": \"YOUR_ACTUAL_LABEL_COLUMN_NAME\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Dry run the import job to make sure that it is set up correctly (optional, but recommended)\n",
    "To learn more about the dry run mechanism, please consult our file importer data ingestion [docs](https://docs.arize.com/arize/data-ingestion/object-store-integration/file-importer-data-ingestion-faq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A re-usable mutation for creating an import job.\n",
    "# Tip: the parameter `dryRun` is set to true in the below mutation to test the import job WITHOUT writing any changes to the server.\n",
    "# No import job will be created as a result of running the below mutation.\n",
    "# When the dry run parameter is set to true, the response of the createFileImportJob mutation will be a validationResult object.\n",
    "\n",
    "create_file_import_job_dry_run = gql(\n",
    "    \"\"\"\n",
    "     mutation createNewImportJob(\n",
    "       $spaceId: ID!, \n",
    "       $modelName: String!,\n",
    "       $modelType: ModelType!,\n",
    "       $modelEnvironmentName: ModelEnvironmentName!,\n",
    "       $blobStore: BlobStoreType!,\n",
    "       $bucketName: String!,\n",
    "       $prefix: String!,\n",
    "       $schema: FileImportSchemaInputType!,\n",
    "     ) {\n",
    "        createFileImportJob(\n",
    "          input: {\n",
    "            spaceId: $spaceId,\n",
    "            modelName: $modelName,\n",
    "            modelType: $modelType,\n",
    "            modelEnvironmentName: $modelEnvironmentName,\n",
    "            blobStore: $blobStore,\n",
    "            bucketName: $bucketName,\n",
    "            prefix: $prefix,\n",
    "            schema: $schema,\n",
    "            dryRun: true,\n",
    "          }\n",
    "        ) {\n",
    "          validationResult { \n",
    "            validationStatus \n",
    "            filePath \n",
    "            error {\n",
    "              message\n",
    "            } \n",
    "          }\n",
    "        }\n",
    "     }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"spaceId\": SPACE_ID,\n",
    "    \"modelName\": MODEL_NAME,\n",
    "    \"modelType\": MODEL_TYPE,\n",
    "    \"modelEnvironmentName\": MODEL_ENVIRONMENT_NAME,\n",
    "    \"blobStore\": BLOB_STORE,\n",
    "    \"bucketName\": BUCKET_NAME,\n",
    "    \"prefix\": PREFIX,\n",
    "    \"schema\": SCHEMA,\n",
    "}\n",
    "\n",
    "result = client.execute(create_file_import_job_dry_run, params)\n",
    "print(\n",
    "    f'The validation status of the dry run: {result[\"createFileImportJob\"][\"validationResult\"][\"validationStatus\"]}'\n",
    ")\n",
    "\n",
    "# print the error if the dry run fails\n",
    "if (\n",
    "    result[\"createFileImportJob\"][\"validationResult\"][\"validationStatus\"]\n",
    "    == \"fail\"\n",
    "):\n",
    "    print(\n",
    "        f'{result[\"createFileImportJob\"][\"validationResult\"][\"filePath\"]}: {result[\"createFileImportJob\"][\"validationResult\"][\"error\"][\"message\"]}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the import job after a successful dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now set the dryRun parameter to false or remove it entirely from the mutation input and create the import job.\n",
    "# When the dryRun parameter is set to false or excluded, response of the createFileImportJob mutation will be a space and job object.\n",
    "\n",
    "create_file_import_job = gql(\n",
    "    \"\"\"\n",
    "     mutation createNewImportJob(\n",
    "       $spaceId: ID!, \n",
    "       $modelName: String!,\n",
    "       $modelType: ModelType!,\n",
    "       $modelEnvironmentName: ModelEnvironmentName!,\n",
    "       $blobStore: BlobStoreType!,\n",
    "       $bucketName: String!,\n",
    "       $prefix: String!,\n",
    "       $schema: FileImportSchemaInputType!,\n",
    "     ) {\n",
    "        createFileImportJob(\n",
    "          input: {\n",
    "            spaceId: $spaceId,\n",
    "            modelName: $modelName,\n",
    "            modelType: $modelType,\n",
    "            modelEnvironmentName: $modelEnvironmentName,\n",
    "            blobStore: $blobStore,\n",
    "            bucketName: $bucketName,\n",
    "            prefix: $prefix,\n",
    "            schema: $schema,\n",
    "          }\n",
    "        ) {\n",
    "          fileImportJob { id }\n",
    "        }\n",
    "     }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"spaceId\": SPACE_ID,\n",
    "    \"modelName\": MODEL_NAME,\n",
    "    \"modelType\": MODEL_TYPE,\n",
    "    \"modelEnvironmentName\": MODEL_ENVIRONMENT_NAME,\n",
    "    \"blobStore\": BLOB_STORE,\n",
    "    \"bucketName\": BUCKET_NAME,\n",
    "    \"prefix\": PREFIX,\n",
    "    \"schema\": SCHEMA,\n",
    "}\n",
    "\n",
    "result = client.execute(create_file_import_job, params)\n",
    "print(f\"âœ… You have successfully created your file import job\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
