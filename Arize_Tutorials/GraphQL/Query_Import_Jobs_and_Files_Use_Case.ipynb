{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for Failed Files of an Import Job Using the Arize GraphQL API\n",
    "\n",
    "Your import jobs contain files which Arize attempts to read and upload into the platform. Sometimes, a file may not be able to be successfully uploaded for some reason. One such reason could be an issue with the formatting of your data in a particular row or column of the file. In order to make sure that all your files are successfully uploaded, you may wish to query an import job that you've created for any failed files. If they exist, the file path and a helpful error message are provided for you, and you can rememdy the issue by fixing the error and rename the file to trigger a re-read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the GraphQL Client using your developer API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gql[all]\n",
    "from gql import Client, gql\n",
    "from gql.transport.requests import RequestsHTTPTransport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your API key\n",
    "First, make sure you have developer permissions. If you are able to visit the [API explorer](https://app.arize.com/graphql), then you have developer permissions. If not, please ask your Account Admin to provide you with access. \n",
    "\n",
    "The API key can be retrieved from the [API explorer](https://app.arize.com/graphql) page. Click the button on the top right called \"Get Your API Key.\" A modal will pop up with your key, copy that into the `API_KEY` constant below. \n",
    "\n",
    "NOTE: this key is different than the SDK key used to send data to Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Select your transport with a defined URL endpoint\n",
    "transport = RequestsHTTPTransport(\n",
    "    url=\"https://app.arize.com/graphql/\", headers={\"x-api-key\": API_KEY}\n",
    ")\n",
    "\n",
    "# Create a GraphQL client using the defined transport\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Execute a GraphQL query to get all your import jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start this query from your space. Spaces have globally unique IDs. You can get your spaceId by visiting app.arize.com.\n",
    "# The url will be in this format: https://app.arize.com/organizations/:orgId/spaces/:spaceId\n",
    "# NOTE: this is not the same as the space key used to send data using the SDK\n",
    "SPACE_ID = \"YOUR_SPACE_ID\"\n",
    "\n",
    "\n",
    "# A re-usable query for fetching your import jobs, a page at a time\n",
    "import_jobs_query = gql(\n",
    "    \"\"\"\n",
    "    query getImportJobs($spaceId: ID!, $cursor: String) {\n",
    "        space: node(id: $spaceId) {\n",
    "            ... on Space {\n",
    "                name\n",
    "                importJobs (first: 50, after: $cursor) {\n",
    "                    pageInfo {\n",
    "                        endCursor\n",
    "                    }\n",
    "                    edges {\n",
    "                        importJob: node {\n",
    "                            id\n",
    "                            modelName\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Base query parameters for fetching import jobs\n",
    "params = {\"spaceId\": SPACE_ID}\n",
    "# An array of models that we will append to\n",
    "import_jobs = []\n",
    "space_name = \"\"\n",
    "\n",
    "\n",
    "# Execute the query on the transport. Continue to pull data until there are no more import jobs\n",
    "while True:\n",
    "    paged_response = client.execute(import_jobs_query, params)\n",
    "    space_name = paged_response[\"space\"][\"name\"]\n",
    "    # Append the import jobs to your list\n",
    "    import_jobs.extend(paged_response[\"space\"][\"importJobs\"][\"edges\"])\n",
    "    # If there is another page of information, point the cursor to the next page and fetch more\n",
    "    end_cursor = paged_response[\"space\"][\"importJobs\"][\"pageInfo\"][\"endCursor\"]\n",
    "    print(\"pageInfo end_cursor %s\" % (end_cursor))\n",
    "    if end_cursor:\n",
    "        print(\"There is another page of import jobs. Loading more.\")\n",
    "        params[\"cursor\"] = end_cursor\n",
    "    else:\n",
    "        print(\"No more import jobs to pull. The list is complete!\")\n",
    "        break\n",
    "\n",
    "print(\"Retrieved {} import jobs\".format(len(import_jobs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a: Print out some of the import jobs to check that exports are expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The import jobs have a nested JSON structure, let's flatten it into a data frame\n",
    "import_jobs_df = pd.json_normalize(import_jobs, sep=\".\")\n",
    "import_jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Save import jobs in a spreadsheet to reference for jobIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gspread\n",
    "# src: https://colab.research.google.com/notebooks/snippets/sheets.ipynb\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "import gspread\n",
    "from google.auth import default\n",
    "\n",
    "creds, _ = default()\n",
    "\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "sheet_name = f\"{space_name} import jobs\"\n",
    "sh = gc.create(sheet_name)\n",
    "\n",
    "# Open our new sheet and add some data.\n",
    "worksheet = gc.open(sheet_name).sheet1\n",
    "\n",
    "# Let's now write the dataframe to google sheets\n",
    "worksheet.update([import_jobs_df.columns.values.tolist()] + import_jobs_df.values.tolist())\n",
    "\n",
    "# print the URL\n",
    "print(sh.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query for failed files on a particular import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start this query from your job. Jobs have globally unique IDs which were queried for in Step 2 above.\n",
    "JOB_ID = \"YOUR_IMPORT_JOB_ID\"\n",
    "\n",
    "\n",
    "# A re-usable query for fetching your files, a page at a time\n",
    "# The status passed into the files connection can be one of FAILED, PENDING, COMPLETE, SKIPPED, or CANCELED. If no status paramter is provided, all files are returned.\n",
    "failed_files_query = gql(\n",
    "    \"\"\"\n",
    "    query getFailedFiles($jobId: ID!, $cursor: String) {\n",
    "        fileImportJob: node(id: $jobId) {\n",
    "            ... on FileImportJob {\n",
    "                modelName\n",
    "                files (first: 50, status: FAILED, after: $cursor) {\n",
    "                    pageInfo {\n",
    "                        endCursor\n",
    "                    }\n",
    "                    edges {\n",
    "                        file: node {\n",
    "                            id\n",
    "                            filePath\n",
    "                            error {\n",
    "                              message\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Base query parameters for fetching import jobs\n",
    "params = {\"jobId\": JOB_ID}\n",
    "# An array of models that we will append to\n",
    "files = []\n",
    "model_name = \"\"\n",
    "\n",
    "\n",
    "# Execute the query on the transport. Continue to pull data until there are no more import jobs\n",
    "while True:\n",
    "    paged_response = client.execute(failed_files_query, params)\n",
    "    space_name = paged_response[\"fileImportJob\"][\"modelName\"]\n",
    "    # Append the import jobs to your list\n",
    "    files.extend(paged_response[\"fileImportJob\"][\"files\"][\"edges\"])\n",
    "    # If there is another page of information, point the cursor to the next page and fetch more\n",
    "    end_cursor = paged_response[\"fileImportJob\"][\"files\"][\"pageInfo\"][\"endCursor\"]\n",
    "    print(\"pageInfo end_cursor %s\" % (end_cursor))\n",
    "    if end_cursor:\n",
    "        print(\"There is another page of files. Loading more.\")\n",
    "        params[\"cursor\"] = end_cursor\n",
    "    else:\n",
    "        print(\"No more files to pull. The list is complete!\")\n",
    "        break\n",
    "\n",
    "print(\"Retrieved {} files\".format(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4a: Print our some of the files to check that exports are expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The import jobs have a nested JSON structure, let's flatten it into a data frame\n",
    "failed_files_df = pd.json_normalize(files, sep=\".\")\n",
    "failed_files_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save files and errors in a spreadsheet for reference in order to fix your errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = f\"{model_name} files\"\n",
    "sh = gc.create(sheet_name)\n",
    "\n",
    "# Open our new sheet and add some data.\n",
    "worksheet = gc.open(sheet_name).sheet1\n",
    "\n",
    "# Let's now write the dataframe to google sheets\n",
    "worksheet.update([failed_files_df.columns.values.tolist()] + failed_files_df.values.tolist())\n",
    "\n",
    "# print the URL\n",
    "print(sh.url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
